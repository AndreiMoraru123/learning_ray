{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a88bf0d1",
   "metadata": {},
   "source": [
    "# Chapter 1: An Overview of Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2dc7e",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_01_overview.ipynb):\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_01_overview.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aca30be",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d03d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[rllib, serve, tune]\"\n",
    "! pip install pyarrow\n",
    "! pip install tensorflow\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2050f",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdce2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755b31b",
   "metadata": {},
   "source": [
    "One of the reasons we need efficient distributed computing is that we're collecting\n",
    "ever more data with a large variety at increasing speeds.\n",
    "The storage systems, data processing and analytics engines that have emerged in\n",
    "the last decade are crucially important to the success of many companies.\n",
    "Interestingly, most \"big data\" technologies are built for and operated by (data)\n",
    "engineers, that are in charge of data collection and processing tasks.\n",
    "The rationale is to free up data scientists to do what they're best at.\n",
    "As a data science practitioner you might want to focus on training complex machine\n",
    "learning models, running efficient hyperparameter selection, building entirely\n",
    "new and custom models or simulations, or serving your models to showcase them.\n",
    "At the same time you simply might _have to_ scale them to a compute cluster.\n",
    "To do that, the distributed system of your choice needs to support all of these\n",
    "fine-grained \"big compute\" tasks, potentially on specialized hardware.\n",
    "Ideally, it also fits into the big data tool chain you're using and is fast enough\n",
    "to meet your latency requirements.\n",
    "In other words, distributed computing has to be powerful and flexible enough for\n",
    "complex data science workloads, and Ray can help you with that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00480805",
   "metadata": {},
   "source": [
    "Python is likely the most popular language for data science today, and it's certainly\n",
    "the one I find the most useful for my daily work.\n",
    "By now it's over 30 years old, but has a still growing and active community.\n",
    "The rich PyData ecosystem is an essential part of a data scientist's toolbox.\n",
    "How can you make sure to scale out your workloads while still leveraging the tools\n",
    "you need?\n",
    "That's a difficult problem, especially since communities can't be forced to just\n",
    "toss their toolbox, or programming language.\n",
    "That means distributed computing tools for data science have to be built for\n",
    "their existing community."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c5a63e",
   "metadata": {},
   "source": [
    "## What is Ray?\n",
    "What I like about Ray is that it checks all the above boxes.\n",
    "It's a flexible, distributed computing framework build for the Python data\n",
    "science community.\n",
    "Ray is easy to get started and keeps simple things simple.\n",
    "Its core API is as lean as it gets and helps you reason effectively about the\n",
    "distributed programs you want to write.\n",
    "You can efficiently parallelize Python programs on your laptop, and run the code\n",
    "you tested locally on a cluster practically without any changes.\n",
    "Its high-level libraries are easy to configure and can seamlessly be used together.\n",
    "Some of them, like Ray's reinforcement learning library, would have a bright future\n",
    "as standalone projects, distributed or not.\n",
    "While Ray's core is built in C++, it's been a Python-first framework since day one,\n",
    "integrates with many important data science tools, and can count\n",
    "on a growing ecosystem.\n",
    "Distributed Python is not new, and Ray is not the first framework in this space\n",
    "(nor will it be the last), but it is special in what it has to offer.\n",
    "Ray is particularly strong when you combine several of its modules and have custom,\n",
    "machine learning heavy workloads that would be difficult to implement otherwise.\n",
    "It makes distributed computing easy enough to run your complex workloads flexibly\n",
    "by leveraging the Python tools you know and want to use.\n",
    "In other words, by _learning Ray_ you get to know _flexible distributed Python for\n",
    "machine learning_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2795bed4",
   "metadata": {},
   "source": [
    "In this chapter you'll get a first glimpse at what Ray can do for you.\n",
    "We will discuss the three layers that make up Ray, namely its core engine,\n",
    "its high-level libraries and its ecosystem.\n",
    "Throughout the chapter we'll show you first code examples to give you a feel for Ray,\n",
    "but we defer any in-depth treatment of Ray's APIs and components to later chapters.\n",
    "You can view this chapter as an overview of the whole book as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a551fa5c",
   "metadata": {},
   "source": [
    "![Ray Layers](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/ray_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4462377",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## A distributed computing framework\n",
    "At its core, Ray is a distributed computing framework.\n",
    "We'll  provide you with just the basic terminology here, and talk about Ray's\n",
    "architecture in depth in chapter 2.\n",
    "In short, Ray sets up and manages clusters of computers so that you can run\n",
    "distributed tasks on them.\n",
    "A ray cluster consists of nodes that are connected to each other via a network.\n",
    "You program against the so-called _driver_, the program root, which lives on\n",
    "the head node_.\n",
    "The driver can run _jobs_, that is a collection of tasks, that are run on the nodes\n",
    "in the cluster.\n",
    "Specifically, the individual tasks of a job are run on _worker_ processes on\n",
    "worker nodes_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4c2640",
   "metadata": {},
   "source": [
    "![Ray cluster](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_01/simple_cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8668e1ae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "What's interesting is that a Ray cluster can also be a _local cluster_, i.e. a cluster\n",
    "consisting just of your own computer.\n",
    "In this case, there's just one node, namely the head node, which has the driver\n",
    "process and some worker processes.\n",
    "\n",
    "With that knowledge at hand, it's time to get your hands dirty and run your first\n",
    "local Ray cluster.\n",
    "Installing Ray on any of the major operating systems should work seamlessly\n",
    "using `pip`:\n",
    "\n",
    "```\n",
    "pip install \"ray[rllib, tune, serve]\"\n",
    "```\n",
    "\n",
    "With a simple `pip install ray` you would have installed just the very basics of Ray.\n",
    "Since we want to explore some advanced features, we installed the \"extras\" `rllib`\n",
    "and `tune`, which we'll discuss in a bit.\n",
    "Depending on your system configuration you may not need the quotation marks in the\n",
    "above installation command.\n",
    "\n",
    "Next, go ahead and start a Python session.\n",
    "You could use the `ipython` interpreter, which I find to be the most suitable\n",
    "environment for following along simple examples.\n",
    "The choice is up to you, but in any case please remember to use Python version\n",
    "`3.7` or later.\n",
    "In your Python session you can now easily import and initialize Ray as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29d6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::init[]\n",
    "import ray\n",
    "ray.init()\n",
    "# end::init[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4312e12f",
   "metadata": {},
   "source": [
    "## Data Processing with Ray Data\n",
    "The first high-level library of Ray we talk about is called \"Ray Data\".\n",
    "This library contains a data structure aptly called `Dataset`, a multitude of\n",
    "connectors for loading data from various formats and systems,\n",
    "an API for transforming such datasets, a way to build data processing pipelines\n",
    "with them, and many integrations with other data processing frameworks.\n",
    "The `Dataset` abstraction builds on the powerful\n",
    "[Arrow framework](https://arrow.apache.org/).\n",
    "\n",
    "To use Ray Data, you need to install Arrow for Python, for instance by running\n",
    "`pip install pyarrow`.\n",
    "We'll now discuss a simple example that creates a distributed `Dataset` on your\n",
    "local Ray cluster from a Python data structure.\n",
    "Specifically, you'll create a dataset from a Python dictionary containing a\n",
    "string `name` and an integer-valued `data` for `10000` entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_data_load[]\n",
    "import ray\n",
    "\n",
    "items = [{\"name\": str(i), \"data\": i} for i in range(10000)]\n",
    "ds = ray.data.from_items(items)   # <1>\n",
    "ds.show(5)  # <2>\n",
    "# end::ray_data_load[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c95752b",
   "metadata": {},
   "source": [
    "Great, now you have some distributed rows, but what can you do with that data?\n",
    "The `Dataset` API bets heavily on functional programming, as it is very well suited\n",
    "for data transformations.\n",
    "Even though Python 3 made a point of hiding some of its functional programming\n",
    "capabilities, you're probably\n",
    "familiar with functionality such as `map`, `filter` and others.\n",
    "If not, it's easy enough to pick up.\n",
    "`map` takes each element of your dataset and transforms is into something\n",
    "else, in parallel.\n",
    "`filter` removes data points according to a boolean filter function.\n",
    "And the slightly more elaborate `flat_map` first maps values similarly to `map`,\n",
    "but then also \"flattens\" the result.\n",
    "For instance, if `map` would produce a list of lists, `flat_map` would flatten out\n",
    "the nested lists and give\n",
    "you just a list.\n",
    "Equipped with these three functional API calls, let's see how easily you can\n",
    "transform your dataset `ds`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e008dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_data_transform[]\n",
    "squares = ds.map(lambda x: x[\"data\"] ** 2)  # <1>\n",
    "\n",
    "evens = squares.filter(lambda x: x % 2 == 0)  # <2>\n",
    "evens.count()\n",
    "\n",
    "cubes = evens.flat_map(lambda x: [x, x**3])  # <3>\n",
    "sample = cubes.take(10)  # <4>\n",
    "print(sample)\n",
    "# end::ray_data_transform[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e42ee9",
   "metadata": {},
   "source": [
    "The drawback of `Dataset` transformations is that each step gets executed\n",
    "synchronously.\n",
    "In the above example this is a non-issue, but for complex tasks that e.g. mix\n",
    "reading files and processing data,\n",
    "you want an execution that can overlap the individual tasks.\n",
    "`DatasetPipeline` does exactly that.\n",
    "Let's rewrite the last example into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_data_pipeline[]\n",
    "pipe = ds.window()  # <1>\n",
    "result = pipe\\\n",
    "    .map(lambda x: x[\"data\"] ** 2)\\\n",
    "    .filter(lambda x: x % 2 == 0)\\\n",
    "    .flat_map(lambda x: [x, x**3])  # <2>\n",
    "result.show(10)\n",
    "# end::ray_data_pipeline[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20a3296",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Reinforcement Learning with Ray RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbc646f",
   "metadata": {},
   "source": [
    "## Distributed training with Ray Train\n",
    "\n",
    "Ray RLlib is dedicated to reinforcement learning, but what do you do if you need\n",
    "to train models for other types of machine learning, like supervised learning?\n",
    "You can use another Ray library for distributed training in this case, called\n",
    "Ray Train_.\n",
    "At this point, we don't have built up enough knowledge of frameworks such as\n",
    "TensorFlow to give you a\n",
    "concrete and informative example for Ray Train.\n",
    "It also doesn't make sense right now to dive into deep learning or explain what\n",
    "Train is, for that matter.\n",
    "We'll discuss this in chapter 6, when it's time to.\n",
    "But we can at least roughly sketch what a distributed training \"wrapper\" for an\n",
    "ML model would look like.\n",
    "A schematic procedure for running distributed deep learning with Ray Train\n",
    "looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4957a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ray_train_sketch[]\n",
    "from ray.train import Trainer\n",
    "\n",
    "\n",
    "def training_function():  # <1>\n",
    "    pass\n",
    "\n",
    "\n",
    "trainer = Trainer(backend=\"tensorflow\", num_workers=4)  # <2>\n",
    "trainer.start()\n",
    "\n",
    "results = trainer.run(training_function)  # <3>\n",
    "trainer.shutdown()\n",
    "# end::ray_train_sketch[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35125e",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with Ray Tune\n",
    "Naming things is hard, but the Ray team hit the spot with _Ray Tune_, which you can\n",
    "use to tune all\n",
    "sorts of parameters.\n",
    "Specifically, it was built to find good hyperparameters for machine learning models.\n",
    "The typical setup is as follows:\n",
    "\n",
    "- You want to run an extremely computationally expensive training function. In ML it's not uncommon\n",
    "  to run training procedures that take days, if not weeks, but let's say you're dealing with just a couple of minutes.\n",
    "- As result of training, you compute a so-called objective function. Usually you either want to maximize\n",
    "  your gains or minimize your losses in terms of performance of your experiment.\n",
    "- The tricky bit is that your training function might depend on certain parameters,\n",
    "  hyperparameters, that influence the value of your objective function.\n",
    "- You may have a hunch what individual hyperparameters should be, but tuning them all can be difficult.\n",
    "  Even if you can restrict these parameters to a sensible range, it's usually prohibitive to test a wide\n",
    "  range of combinations. Your training function is simply too expensive.\n",
    "\n",
    "What can you do to efficiently sample hyperparameters and get \"good enough\" results on your objective?\n",
    "The field concerned with solving this problem is called _hyperparameter optimization_ (HPO), and Ray Tune has\n",
    "an enormous suite of algorithms for tackling it.\n",
    "Let's look at a first example of Ray Tune used for the situation we just explained.\n",
    "The focus is yet again on Ray and its API, and not on a specific ML task (which we simply simulate for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c4f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_tune[]\n",
    "from ray import tune\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "def training_function(config):  # <1>\n",
    "    x, y = config[\"x\"], config[\"y\"]\n",
    "    time.sleep(10)\n",
    "    score = objective(x, y)\n",
    "    tune.report(score=score)  # <2>\n",
    "\n",
    "\n",
    "def objective(x, y):\n",
    "    return math.sqrt((x**2 + y**2)/2)  # <3>\n",
    "\n",
    "\n",
    "result = tune.run(  # <4>\n",
    "    training_function,\n",
    "    config={\n",
    "        \"x\": tune.grid_search([-1, -.5, 0, .5, 1]),  # <5>\n",
    "        \"y\": tune.grid_search([-1, -.5, 0, .5, 1])\n",
    "    })\n",
    "\n",
    "print(result.get_best_config(metric=\"score\", mode=\"min\"))\n",
    "# end::ray_tune[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b0c80",
   "metadata": {},
   "source": [
    "## Model Serving with Ray Serve\n",
    "\n",
    "The last of Ray's high-level libraries we'll discuss specializes on model serving and is simply called _Ray Serve_.\n",
    "To see an example of it in action, you need a trained ML model to serve.\n",
    "Luckily, nowadays you can find many interesting models on the internet that have already been trained for you.\n",
    "For instance, _Hugging Face_ has a variety of models available for you to download directly in Python.\n",
    "The model we'll use is a language model called _GPT-2_ that takes text as input and produces text to\n",
    "continue or complete the input.\n",
    "For example, you can prompt a question and GPT-2 will try to complete it.\n",
    "\n",
    "Serving such a model is a good way to make it accessible.\n",
    "You may not now how to load and run a TensorFlow model on your computer, but you do now how\n",
    "to ask a question in plain English.\n",
    "Model serving hides the implementation details of a solution and lets users focus on providing\n",
    "inputs and understanding outputs of a model.\n",
    "\n",
    "To proceed, make sure to run `pip install transformers` to install the Hugging Face library\n",
    "that has the model we want to use.\n",
    "With that we can now import and start an instance of Ray's `serve` library, load and deploy a GPT-2\n",
    "model and ask it for the meaning of life, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb8ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_serve[]\n",
    "from ray import serve\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "\n",
    "serve.start()  # <1>\n",
    "\n",
    "\n",
    "@serve.deployment  # <2>\n",
    "def model(request):\n",
    "    language_model = pipeline(\"text-generation\", model=\"gpt2\")  # <3>\n",
    "    query = request.query_params[\"query\"]\n",
    "    return language_model(query, max_length=100)  # <4>\n",
    "\n",
    "\n",
    "model.deploy()  # <5>\n",
    "\n",
    "query = \"What's the meaning of life?\"\n",
    "response = requests.get(f\"http://localhost:8000/model?query={query}\")  # <6>\n",
    "print(response.text)\n",
    "# end::ray_serve[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
