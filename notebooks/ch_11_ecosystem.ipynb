{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfd352a6",
   "metadata": {},
   "source": [
    "# Chapter 11: Ray's Ecosystem and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1625d",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_11_ecosystem.ipynb).\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a29bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18c1823",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84c8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[air]\"==2.2.0\n",
    "! pip install requests torch torchvision mlflow gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f35a22",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4587afa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::load_cifar[]\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "\n",
    "def load_cifar(train: bool):\n",
    "    transform = transforms.Compose([  # <1>\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    return datasets.CIFAR10(  # <2>\n",
    "        root=\"./data\",\n",
    "        download=True,\n",
    "        train=train,  # <3>\n",
    "        transform=transform\n",
    "    )\n",
    "# end::load_cifar[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430187ce",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::std[]\n",
    "from ray.data import from_torch\n",
    "\n",
    "\n",
    "train_dataset = from_torch(load_cifar(train=True))\n",
    "test_dataset = from_torch(load_cifar(train=False))\n",
    "# end::std[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e593fac",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::batch_conversion[]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def to_labeled_image(batch):  # <1>\n",
    "    return {\n",
    "        \"image\": np.array([image.numpy() for image, _ in batch]),\n",
    "        \"label\": np.array([label for _, label in batch]),\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map_batches(to_labeled_image)  # <2>\n",
    "test_dataset = test_dataset.map_batches(to_labeled_image)\n",
    "# end::batch_conversion[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b9f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::torch_model[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# end::torch_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d53f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::torch_training_loop[]\n",
    "from ray import train\n",
    "from ray.air import session, Checkpoint\n",
    "\n",
    "\n",
    "def train_loop(config):\n",
    "    model = train.torch.prepare_model(Net())  # <1>\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    train_batches = session.get_dataset_shard(\"train\").iter_torch_batches(  # <2>\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_batches):\n",
    "            inputs, labels = data[\"image\"], data[\"label\"]  # <3>\n",
    "\n",
    "            optimizer.zero_grad()  # <4>\n",
    "            forward_outputs = model(inputs)\n",
    "            loss = loss_fct(forward_outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()  # <5>\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"[{epoch + 1}, {i + 1:4d}] loss: \"\n",
    "                      f\"{running_loss / 1000:.3f}\")\n",
    "                running_loss = 0.0\n",
    "\n",
    "        session.report(  # <6>\n",
    "            dict(running_loss=running_loss),\n",
    "            checkpoint=Checkpoint.from_dict(\n",
    "                dict(model=model.module.state_dict())\n",
    "            ),\n",
    "        )\n",
    "# end::torch_training_loop[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad9d9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::torch_trainer[]\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "from ray.air.callbacks.mlflow import MLflowLoggerCallback\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop,\n",
    "    train_loop_config={\"batch_size\": 10, \"epochs\": 5},\n",
    "    datasets={\"train\": train_dataset},\n",
    "    scaling_config=ScalingConfig(num_workers=2),\n",
    "    run_config=RunConfig(callbacks=[\n",
    "        MLflowLoggerCallback(experiment_name=\"torch_trainer\")\n",
    "    ])\n",
    "\n",
    ")\n",
    "result = trainer.fit()\n",
    "# end::torch_trainer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ef0e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::store_checkpoint[]\n",
    "CHECKPOINT_PATH = \"torch_checkpoint\"\n",
    "result.checkpoint.to_directory(CHECKPOINT_PATH)\n",
    "# end::store_checkpoint[]\n",
    "\n",
    "import sys\n",
    "sys.exit(\"End of executable script. Stopping smoke tests here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e364b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::custom_data[]\n",
    "from ray.data import read_datasource, datasource\n",
    "\n",
    "\n",
    "class SnowflakeDatasource(datasource.Datasource):\n",
    "    pass\n",
    "\n",
    "\n",
    "dataset = read_datasource(SnowflakeDatasource(), ...)\n",
    "# end::custom_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0516f9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::custom_trainer[]\n",
    "from ray.train.data_parallel_trainer import DataParallelTrainer\n",
    "\n",
    "\n",
    "class JaxTrainer(DataParallelTrainer):\n",
    "    pass\n",
    "\n",
    "\n",
    "trainer = JaxTrainer(\n",
    "    ...,\n",
    "    scaling_config=ScalingConfig(...),\n",
    "    datasets=dict(train=dataset),\n",
    ")\n",
    "# end::custom_trainer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e08a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::custom_tuner[]\n",
    "from ray.tune import logger, tuner\n",
    "from ray.air.config import RunConfig\n",
    "\n",
    "\n",
    "class NeptuneCallback(logger.LoggerCallback):\n",
    "    pass\n",
    "\n",
    "\n",
    "tuner = tuner.Tuner(\n",
    "    trainer,\n",
    "    run_config=RunConfig(callbacks=[NeptuneCallback()])\n",
    ")\n",
    "# end::custom_tuner[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
