{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "780507ca",
   "metadata": {},
   "source": [
    "# Chapter 4: Reinforcement Learning with Ray RLlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2856c7b0",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_04_rllib.ipynb).\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5e1f7",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d6d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[rllib]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17e3aa",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8206edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c77f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::gym_mock[]\n",
    "import gym\n",
    "\n",
    "\n",
    "class Env:\n",
    "\n",
    "    action_space: gym.spaces.Space\n",
    "    observation_space: gym.spaces.Space  # <1>\n",
    "\n",
    "    def step(self, action):  # <2>\n",
    "        ...\n",
    "\n",
    "    def reset(self):  # <3>\n",
    "        ...\n",
    "\n",
    "    def render(self, mode=\"human\"):  # <4>\n",
    "        ...\n",
    "# end::gym_mock[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965ee003",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::rllib_dqn_simple[]\n",
    "from ray.tune.logger import pretty_print\n",
    "from maze_gym_env import GymEnvironment\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "\n",
    "config = DQNConfig().environment(GymEnvironment).rollouts(num_rollout_workers=4)\n",
    "pretty_print(config.to_dict())\n",
    "\n",
    "algo = config.build()  # <1>\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()  # <2>\n",
    "\n",
    "print(pretty_print(result))  # <3>\n",
    "# end::rllib_dqn_simple[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283bf0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::rllib_simple_save[]\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "\n",
    "checkpoint = algo.save()  # <1>\n",
    "print(checkpoint)\n",
    "\n",
    "restored_algorithm = Algorithm.from_checkpoint(checkpoint)  # <2>\n",
    "\n",
    "evaluation = algo.evaluate()  # <3>\n",
    "print(pretty_print(evaluation))\n",
    "\n",
    "# end::rllib_simple_save[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e3c637",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::rllib_manual_rollout[]\n",
    "env = GymEnvironment()\n",
    "done = False\n",
    "total_reward = 0\n",
    "observations = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = algo.compute_single_action(observations)  # <1>\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "# end::rllib_manual_rollout[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::rllib_actions[]\n",
    "action = algo.compute_actions(  # <1>\n",
    "    {\"obs_1\": observations, \"obs_2\": observations}\n",
    ")\n",
    "print(action)\n",
    "# {'obs_1': 0, 'obs_2': 1}\n",
    "# end::rllib_actions[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d2c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::rllib_policy[]\n",
    "policy = algo.get_policy()\n",
    "print(policy.get_weights())\n",
    "\n",
    "model = policy.model\n",
    "# end::rllib_policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83366455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::rllib_workers[]\n",
    "workers = algo.workers\n",
    "workers.foreach_worker(\n",
    "    lambda remote_trainer: remote_trainer.get_policy().get_weights()\n",
    ")\n",
    "# end::rllib_workers[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e167b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::rllib_q_network[]\n",
    "model.base_model.summary()\n",
    "\n",
    "# end::rllib_q_network[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a539340",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::rllib_model_output[]\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "\n",
    "env = GymEnvironment()\n",
    "obs_space = env.observation_space\n",
    "preprocessor = get_preprocessor(obs_space)(obs_space)  # <1>\n",
    "\n",
    "observations = env.reset()\n",
    "transformed = preprocessor.transform(observations).reshape(1, -1)  # <2>\n",
    "\n",
    "model_output, _ = model.from_batch({\"obs\": transformed})  # <3>\n",
    "# end::rllib_model_output[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f47a55f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::rllib_q_values_action_dist[]\n",
    "q_values = model.get_q_value_distributions(model_output)  # <1>\n",
    "print(q_values)\n",
    "\n",
    "action_distribution = policy.dist_class(model_output, model)  # <2>\n",
    "sample = action_distribution.sample()  # <3>\n",
    "print(sample)\n",
    "# end::rllib_q_values_action_dist[]\n",
    "\n",
    "# model.get_state_value(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9081e6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::multi_agent_init[]\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete\n",
    "import os\n",
    "\n",
    "\n",
    "class MultiAgentMaze(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self,  *args, **kwargs):  # <1>\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}  # <2>\n",
    "        self.goal = (4, 4)\n",
    "        self.info = {1: {'obs': self.agents[1]}, 2: {'obs': self.agents[2]}}  # <3>\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
    "\n",
    "        return {1: self.get_observation(1), 2: self.get_observation(2)}  # <4>\n",
    "# end::multi_agent_init[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2502774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::multi_agent_helpers[]\n",
    "    def get_observation(self, agent_id):\n",
    "        seeker = self.agents[agent_id]\n",
    "        return 5 * seeker[0] + seeker[1]\n",
    "\n",
    "    def get_reward(self, agent_id):\n",
    "        return 1 if self.agents[agent_id] == self.goal else 0\n",
    "\n",
    "    def is_done(self, agent_id):\n",
    "        return self.agents[agent_id] == self.goal\n",
    "# end::multi_agent_helpers[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::multi_agent_step[]\n",
    "    def step(self, action):  # <1>\n",
    "        agent_ids = action.keys()\n",
    "\n",
    "        for agent_id in agent_ids:\n",
    "            seeker = self.agents[agent_id]\n",
    "            if action[agent_id] == 0:  # move down\n",
    "                seeker = (min(seeker[0] + 1, 4), seeker[1])\n",
    "            elif action[agent_id] == 1:  # move left\n",
    "                seeker = (seeker[0], max(seeker[1] - 1, 0))\n",
    "            elif action[agent_id] == 2:  # move up\n",
    "                seeker = (max(seeker[0] - 1, 0), seeker[1])\n",
    "            elif action[agent_id] == 3:  # move right\n",
    "                seeker = (seeker[0], min(seeker[1] + 1, 4))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "            self.agents[agent_id] = seeker  # <2>\n",
    "\n",
    "        observations = {i: self.get_observation(i) for i in agent_ids}  # <3>\n",
    "        rewards = {i: self.get_reward(i) for i in agent_ids}\n",
    "        done = {i: self.is_done(i) for i in agent_ids}\n",
    "\n",
    "        done[\"__all__\"] = all(done.values())  # <4>\n",
    "\n",
    "        return observations, rewards, done, self.info\n",
    "# end::multi_agent_step[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dfdf0a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::multi_agent_render[]\n",
    "    def render(self, *args, **kwargs):\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.agents[1][0]][self.agents[1][1]] = '|1'\n",
    "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "# end::multi_agent_render[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b74fd5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::multi_agent_run[]\n",
    "import time\n",
    "\n",
    "env = MultiAgentMaze()\n",
    "\n",
    "while True:\n",
    "    obs, rew, done, info = env.step(\n",
    "        {1: env.action_space.sample(), 2: env.action_space.sample()}\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    if any(done.values()):\n",
    "        break\n",
    "# end::multi_agent_run[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417642b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::multi_agent_simple[]\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "simple_trainer = DQNConfig().environment(env=MultiAgentMaze)\n",
    "simple_trainer.train()\n",
    "# end::multi_agent_simple[]\n",
    "\n",
    "# tag::multi_agent_mapping[]\n",
    "algo = DQNConfig()\\\n",
    "    .environment(env=MultiAgentMaze)\\\n",
    "    .multi_agent(\n",
    "        policies={  # <1>\n",
    "            \"policy_1\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
    "            ),\n",
    "            \"policy_2\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",  # <2>\n",
    "    ).build()\n",
    "\n",
    "print(algo.train())\n",
    "# end::multi_agent_mapping[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b4d78",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# tag::advanced_env_init[]\n",
    "from gym.spaces import Discrete\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "class AdvancedEnv(GymEnvironment):\n",
    "\n",
    "    def __init__(self, seeker=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.maze_len = 11\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(self.maze_len * self.maze_len)\n",
    "\n",
    "        if seeker:  # <1>\n",
    "            assert 0 <= seeker[0] < self.maze_len and \\\n",
    "                   0 <= seeker[1] < self.maze_len\n",
    "            self.seeker = seeker\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "        self.goal = (self.maze_len-1, self.maze_len-1)\n",
    "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
    "\n",
    "        self.punish_states = [  # <2>\n",
    "            (i, j) for i in range(self.maze_len) for j in range(self.maze_len)\n",
    "            if i % 2 == 1 and j % 2 == 0\n",
    "        ]\n",
    "# end::advanced_env_init[]\n",
    "\n",
    "# tag::advanced_env_rest[]\n",
    "    def reset(self):\n",
    "        \"\"\"Reset seeker position randomly, return observations.\"\"\"\n",
    "        self.seeker = (\n",
    "            random.randint(0, self.maze_len - 1),\n",
    "            random.randint(0, self.maze_len - 1)\n",
    "        )\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return self.maze_len * self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal and punish forbidden states\"\"\"\n",
    "        reward = -1 if self.seeker in self.punish_states else 0\n",
    "        reward += 5 if self.seeker == self.goal else 0\n",
    "        return reward\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        grid = [['| ' for _ in range(self.maze_len)] +\n",
    "                [\"|\\n\"] for _ in range(self.maze_len)]\n",
    "        for punish in self.punish_states:\n",
    "            grid[punish[0]][punish[1]] = '|X'\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "# end::advanced_env_rest[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::task_settable[]\n",
    "from ray.rllib.env.apis.task_settable_env import TaskSettableEnv\n",
    "\n",
    "\n",
    "class CurriculumEnv(AdvancedEnv, TaskSettableEnv):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AdvancedEnv.__init__(self)\n",
    "\n",
    "    def difficulty(self):  # <1>\n",
    "        return abs(self.seeker[0] - self.goal[0]) + \\\n",
    "               abs(self.seeker[1] - self.goal[1])\n",
    "\n",
    "    def get_task(self):  # <2>\n",
    "        return self.difficulty()\n",
    "\n",
    "    def set_task(self, task_difficulty):  # <3>\n",
    "        while not self.difficulty() <= task_difficulty:\n",
    "            self.reset()\n",
    "# end::task_settable[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dcd250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::curriculum_fn[]\n",
    "def curriculum_fn(train_results, task_settable_env, env_ctx):\n",
    "    time_steps = train_results.get(\"timesteps_total\")\n",
    "    difficulty = time_steps // 1000\n",
    "    print(f\"Current difficulty: {difficulty}\")\n",
    "    return difficulty\n",
    "# end::curriculum_fn[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db20214",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::curriculum_trainer[]\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "import tempfile\n",
    "\n",
    "\n",
    "temp = tempfile.mkdtemp()  # <1>\n",
    "\n",
    "trainer = (\n",
    "    DQNConfig()\n",
    "    .environment(env=CurriculumEnv, env_task_fn=curriculum_fn)  # <2>\n",
    "    .offline_data(output=temp)  # <3>\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(15):\n",
    "    trainer.train()\n",
    "# end::curriculum_trainer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e878b6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::input_trainer[]\n",
    "imitation_algo = (\n",
    "    DQNConfig()\n",
    "    .environment(env=AdvancedEnv)\n",
    "    .offline_data(input_=temp, input_evaluation=[])\n",
    "    .exploration(explore=False)\n",
    "    .build())\n",
    "\n",
    "for i in range(10):\n",
    "    imitation_algo.train()\n",
    "\n",
    "imitation_algo.evaluate()\n",
    "# end::input_trainer[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
