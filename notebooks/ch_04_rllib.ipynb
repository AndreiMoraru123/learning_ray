{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90efb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::gym_mock[]\n",
    "import gym\n",
    "\n",
    "\n",
    "class Env:\n",
    "\n",
    "    action_space: gym.spaces.Space\n",
    "    observation_space: gym.spaces.Space  # <1>\n",
    "\n",
    "    def step(self, action):  # <2>\n",
    "        ...\n",
    "\n",
    "    def reset(self):  # <3>\n",
    "        ...\n",
    "\n",
    "    def render(self, mode=\"human\"):  # <4>\n",
    "        ...\n",
    "# end::gym_mock[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::rllib_dqn_simple[]\n",
    "from ray.tune.logger import pretty_print\n",
    "from maze_gym_env import GymEnvironment\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "\n",
    "config = DQNConfig().environment(GymEnvironment).rollouts(num_rollout_workers=4)\n",
    "pretty_print(config.to_dict())\n",
    "\n",
    "algo = config.build()  # <1>\n",
    "\n",
    "for i in range(10):\n",
    "    result = algo.train()  # <2>\n",
    "\n",
    "print(pretty_print(result))  # <3>\n",
    "# end::rllib_dqn_simple[]\n",
    "\n",
    "\n",
    "# tag::rllib_simple_save[]\n",
    "from ray.rllib.algorithms.algorithm import Algorithm\n",
    "\n",
    "\n",
    "checkpoint = algo.save()  # <1>\n",
    "print(checkpoint)\n",
    "\n",
    "restored_algorithm = Algorithm.from_checkpoint(checkpoint)  # <2>\n",
    "\n",
    "evaluation = algo.evaluate()  # <3>\n",
    "print(pretty_print(evaluation))\n",
    "\n",
    "# end::rllib_simple_save[]\n",
    "\n",
    "# TODO: if I pretty print in the loop above, the \"evaluation\" has only NaNs.\n",
    "\n",
    "# tag::rllib_manual_rollout[]\n",
    "env = GymEnvironment()\n",
    "done = False\n",
    "total_reward = 0\n",
    "observations = env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = algo.compute_single_action(observations)  # <1>\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "# end::rllib_manual_rollout[]\n",
    "\n",
    "# tag::rllib_actions[]\n",
    "action = algo.compute_actions(  # <1>\n",
    "    {\"obs_1\": observations, \"obs_2\": observations}\n",
    ")\n",
    "print(action)\n",
    "# {'obs_1': 0, 'obs_2': 1}\n",
    "# end::rllib_actions[]\n",
    "\n",
    "# tag::rllib_policy[]\n",
    "policy = algo.get_policy()\n",
    "print(policy.get_weights())\n",
    "\n",
    "model = policy.model\n",
    "# end::rllib_policy[]\n",
    "\n",
    "# tag::rllib_workers[]\n",
    "workers = algo.workers\n",
    "workers.foreach_worker(\n",
    "    lambda remote_trainer: remote_trainer.get_policy().get_weights()\n",
    ")\n",
    "# end::rllib_workers[]\n",
    "\n",
    "# tag::rllib_q_network[]\n",
    "model.base_model.summary()\n",
    "\n",
    "# end::rllib_q_network[]\n",
    "\n",
    "# tag::rllib_model_output[]\n",
    "from ray.rllib.models.preprocessors import get_preprocessor\n",
    "\n",
    "\n",
    "env = GymEnvironment()\n",
    "obs_space = env.observation_space\n",
    "preprocessor = get_preprocessor(obs_space)(obs_space)  # <1>\n",
    "\n",
    "observations = env.reset()\n",
    "transformed = preprocessor.transform(observations).reshape(1, -1)  # <2>\n",
    "\n",
    "model_output, _ = model.from_batch({\"obs\": transformed})  # <3>\n",
    "# end::rllib_model_output[]\n",
    "\n",
    "# tag::rllib_q_values_action_dist[]\n",
    "q_values = model.get_q_value_distributions(model_output)  # <1>\n",
    "print(q_values)\n",
    "\n",
    "action_distribution = policy.dist_class(model_output, model)  # <2>\n",
    "sample = action_distribution.sample()  # <3>\n",
    "print(sample)\n",
    "# end::rllib_q_values_action_dist[]\n",
    "\n",
    "# model.get_state_value(model_output)\n",
    "\n",
    "# tag::multi_agent_init[]\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from gym.spaces import Discrete\n",
    "import os\n",
    "\n",
    "\n",
    "class MultiAgentMaze(MultiAgentEnv):\n",
    "\n",
    "    def __init__(self,  *args, **kwargs):  # <1>\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}  # <2>\n",
    "        self.goal = (4, 4)\n",
    "        self.info = {1: {'obs': self.agents[1]}, 2: {'obs': self.agents[2]}}  # <3>\n",
    "\n",
    "    def reset(self):\n",
    "        self.agents = {1: (4, 0), 2: (0, 4)}\n",
    "\n",
    "        return {1: self.get_observation(1), 2: self.get_observation(2)}  # <4>\n",
    "# end::multi_agent_init[]\n",
    "\n",
    "# tag::multi_agent_helpers[]\n",
    "    def get_observation(self, agent_id):\n",
    "        seeker = self.agents[agent_id]\n",
    "        return 5 * seeker[0] + seeker[1]\n",
    "\n",
    "    def get_reward(self, agent_id):\n",
    "        return 1 if self.agents[agent_id] == self.goal else 0\n",
    "\n",
    "    def is_done(self, agent_id):\n",
    "        return self.agents[agent_id] == self.goal\n",
    "# end::multi_agent_helpers[]\n",
    "\n",
    "# tag::multi_agent_step[]\n",
    "    def step(self, action):  # <1>\n",
    "        agent_ids = action.keys()\n",
    "\n",
    "        for agent_id in agent_ids:\n",
    "            seeker = self.agents[agent_id]\n",
    "            if action[agent_id] == 0:  # move down\n",
    "                seeker = (min(seeker[0] + 1, 4), seeker[1])\n",
    "            elif action[agent_id] == 1:  # move left\n",
    "                seeker = (seeker[0], max(seeker[1] - 1, 0))\n",
    "            elif action[agent_id] == 2:  # move up\n",
    "                seeker = (max(seeker[0] - 1, 0), seeker[1])\n",
    "            elif action[agent_id] == 3:  # move right\n",
    "                seeker = (seeker[0], min(seeker[1] + 1, 4))\n",
    "            else:\n",
    "                raise ValueError(\"Invalid action\")\n",
    "            self.agents[agent_id] = seeker  # <2>\n",
    "\n",
    "        observations = {i: self.get_observation(i) for i in agent_ids}  # <3>\n",
    "        rewards = {i: self.get_reward(i) for i in agent_ids}\n",
    "        done = {i: self.is_done(i) for i in agent_ids}\n",
    "\n",
    "        done[\"__all__\"] = all(done.values())  # <4>\n",
    "\n",
    "        return observations, rewards, done, self.info\n",
    "# end::multi_agent_step[]\n",
    "\n",
    "# tag::multi_agent_render[]\n",
    "    def render(self, *args, **kwargs):\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.agents[1][0]][self.agents[1][1]] = '|1'\n",
    "        grid[self.agents[2][0]][self.agents[2][1]] = '|2'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "# end::multi_agent_render[]\n",
    "\n",
    "\n",
    "# tag::multi_agent_run[]\n",
    "import time\n",
    "\n",
    "env = MultiAgentMaze()\n",
    "\n",
    "while True:\n",
    "    obs, rew, done, info = env.step(\n",
    "        {1: env.action_space.sample(), 2: env.action_space.sample()}\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    env.render()\n",
    "    if any(done.values()):\n",
    "        break\n",
    "# end::multi_agent_run[]\n",
    "\n",
    "# tag::multi_agent_simple[]\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "\n",
    "simple_trainer = DQNConfig().environment(env=MultiAgentMaze)\n",
    "simple_trainer.train()\n",
    "# end::multi_agent_simple[]\n",
    "\n",
    "# tag::multi_agent_mapping[]\n",
    "algo = DQNConfig()\\\n",
    "    .environment(env=MultiAgentMaze)\\\n",
    "    .multi_agent(\n",
    "        policies={  # <1>\n",
    "            \"policy_1\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.80}\n",
    "            ),\n",
    "            \"policy_2\": (\n",
    "                None, env.observation_space, env.action_space, {\"gamma\": 0.95}\n",
    "            ),\n",
    "        },\n",
    "        policy_mapping_fn = lambda agent_id: f\"policy_{agent_id}\",  # <2>\n",
    "    ).build()\n",
    "\n",
    "print(algo.train())\n",
    "# end::multi_agent_mapping[]\n",
    "\n",
    "\n",
    "# tag::advanced_env_init[]\n",
    "from gym.spaces import Discrete\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "class AdvancedEnv(GymEnvironment):\n",
    "\n",
    "    def __init__(self, seeker=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.maze_len = 11\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(self.maze_len * self.maze_len)\n",
    "\n",
    "        if seeker:  # <1>\n",
    "            assert 0 <= seeker[0] < self.maze_len and \\\n",
    "                   0 <= seeker[1] < self.maze_len\n",
    "            self.seeker = seeker\n",
    "        else:\n",
    "            self.reset()\n",
    "\n",
    "        self.goal = (self.maze_len-1, self.maze_len-1)\n",
    "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
    "\n",
    "        self.punish_states = [  # <2>\n",
    "            (i, j) for i in range(self.maze_len) for j in range(self.maze_len)\n",
    "            if i % 2 == 1 and j % 2 == 0\n",
    "        ]\n",
    "# end::advanced_env_init[]\n",
    "\n",
    "# tag::advanced_env_rest[]\n",
    "    def reset(self):\n",
    "        \"\"\"Reset seeker position randomly, return observations.\"\"\"\n",
    "        self.seeker = (\n",
    "            random.randint(0, self.maze_len - 1),\n",
    "            random.randint(0, self.maze_len - 1)\n",
    "        )\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return self.maze_len * self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal and punish forbidden states\"\"\"\n",
    "        reward = -1 if self.seeker in self.punish_states else 0\n",
    "        reward += 5 if self.seeker == self.goal else 0\n",
    "        return reward\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        grid = [['| ' for _ in range(self.maze_len)] +\n",
    "                [\"|\\n\"] for _ in range(self.maze_len)]\n",
    "        for punish in self.punish_states:\n",
    "            grid[punish[0]][punish[1]] = '|X'\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))\n",
    "# end::advanced_env_rest[]\n",
    "\n",
    "\n",
    "# tag::task_settable[]\n",
    "from ray.rllib.env.apis.task_settable_env import TaskSettableEnv\n",
    "\n",
    "\n",
    "class CurriculumEnv(AdvancedEnv, TaskSettableEnv):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        AdvancedEnv.__init__(self)\n",
    "\n",
    "    def difficulty(self):  # <1>\n",
    "        return abs(self.seeker[0] - self.goal[0]) + \\\n",
    "               abs(self.seeker[1] - self.goal[1])\n",
    "\n",
    "    def get_task(self):  # <2>\n",
    "        return self.difficulty()\n",
    "\n",
    "    def set_task(self, task_difficulty):  # <3>\n",
    "        while not self.difficulty() <= task_difficulty:\n",
    "            self.reset()\n",
    "# end::task_settable[]\n",
    "\n",
    "\n",
    "# tag::curriculum_fn[]\n",
    "def curriculum_fn(train_results, task_settable_env, env_ctx):\n",
    "    time_steps = train_results.get(\"timesteps_total\")\n",
    "    difficulty = time_steps // 1000\n",
    "    print(f\"Current difficulty: {difficulty}\")\n",
    "    return difficulty\n",
    "# end::curriculum_fn[]\n",
    "\n",
    "\n",
    "# tag::curriculum_trainer[]\n",
    "from ray.rllib.algorithms.dqn import DQNConfig\n",
    "import tempfile\n",
    "\n",
    "\n",
    "temp = tempfile.mkdtemp()  # <1>\n",
    "\n",
    "trainer = (\n",
    "    DQNConfig()\n",
    "    .environment(env=CurriculumEnv, env_task_fn=curriculum_fn)  # <2>\n",
    "    .offline_data(output=temp)  # <3>\n",
    "    .build()\n",
    ")\n",
    "\n",
    "for i in range(15):\n",
    "    trainer.train()\n",
    "# end::curriculum_trainer[]\n",
    "\n",
    "# tag::input_trainer[]\n",
    "imitation_algo = (\n",
    "    DQNConfig()\n",
    "    .environment(env=AdvancedEnv)\n",
    "    .offline_data(input_=temp, input_evaluation=[])\n",
    "    .exploration(explore=False)\n",
    "    .build())\n",
    "\n",
    "for i in range(10):\n",
    "    imitation_algo.train()\n",
    "\n",
    "imitation_algo.evaluate()\n",
    "# end::input_trainer[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
