{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89466d94",
   "metadata": {},
   "source": [
    "# Chapter 7: Distributed Training with Ray Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c23b3af",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_07_train.ipynb).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_07_train.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5daa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e05d4f",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1765ed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[data,train]\"==2.1.0 dask==2022.2.0 torch==1.12.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ef63a5",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a179146",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409cb5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_preprocess[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "LABEL_COLUMN = \"is_big_tip\"\n",
    "FEATURE_COLUMNS = [\"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "                   \"trip_duration\", \"hour\", \"day_of_week\"]\n",
    "\n",
    "enable_dask_on_ray()\n",
    "\n",
    "\n",
    "def load_dataset(path: str, *, include_label=True):\n",
    "    columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\",\n",
    "               \"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
    "    df = dd.read_parquet(path, columns=columns)  # <1>\n",
    "\n",
    "    df = df.dropna()  # <2>\n",
    "    df = df[(df[\"passenger_count\"] <= 4) &\n",
    "            (df[\"trip_distance\"] < 100) &\n",
    "            (df[\"fare_amount\"] < 1000)]\n",
    "\n",
    "    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n",
    "                           df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n",
    "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday  # <3>\n",
    "\n",
    "    if include_label:\n",
    "        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]  # <4>\n",
    "\n",
    "    df = df.drop(  # <5>\n",
    "        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n",
    "    )\n",
    "\n",
    "    return ray.data.from_dask(df).repartition(100)  # <6>\n",
    "# end::ml_pipeline_preprocess[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91e9d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_model[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FarePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(6, 256)\n",
    "        self.fc2 = nn.Linear(256, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "# end::ml_pipeline_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd15858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_1[]\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config: dict):  # <1>\n",
    "    batch_size = config.get(\"batch_size\", 32)\n",
    "    lr = config.get(\"lr\", 1e-2)\n",
    "    num_epochs = config.get(\"num_epochs\", 3)\n",
    "\n",
    "    dataset_shard = session.get_dataset_shard(\"train\")  # <2>\n",
    "\n",
    "    model = FarePredictor()\n",
    "    dist_model = train.torch.prepare_model(model)  # <3>\n",
    "\n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(dist_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):  # <4>\n",
    "        loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in dataset_shard.iter_torch_batches(  # <5>\n",
    "                batch_size=batch_size, dtypes=torch.float\n",
    "        ):\n",
    "            labels = torch.unsqueeze(batch[LABEL_COLUMN], dim=1)\n",
    "            inputs = torch.cat(\n",
    "                [torch.unsqueeze(batch[f], dim=1) for f in FEATURE_COLUMNS], dim=1\n",
    "            )\n",
    "            output = dist_model(inputs)\n",
    "            batch_loss = loss_function(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "        session.report(  # <6>\n",
    "            {\"epoch\": epoch, \"loss\": loss},\n",
    "            checkpoint=TorchCheckpoint.from_model(dist_model)\n",
    "        )\n",
    "# end::ml_pipeline_train_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209607a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# NOTE: Colab does not have enough resources to run this example.\n",
    "# try using num_workers=1, resources_per_worker={\"CPU\": 1, \"GPU\": 0} in your\n",
    "# ScalingConfig below.\n",
    "# In any case, this training loop will take considerable time to run.\n",
    "# tag::ml_pipeline_train_2[]\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,  # <1>\n",
    "    train_loop_config={  # <2>\n",
    "        \"lr\": 1e-2, \"num_epochs\": 3, \"batch_size\": 64\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=2),  # <3>\n",
    "    datasets={  # <4>\n",
    "        \"train\": load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.parquet\")\n",
    "    },\n",
    ")\n",
    "\n",
    "result = trainer.fit()  # <5>\n",
    "trained_model = result.checkpoint\n",
    "# end::ml_pipeline_train_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88deaa0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_inference[]\n",
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor(trained_model, TorchPredictor)\n",
    "ds = load_dataset(\n",
    "    \"nyc_tlc_data/yellow_tripdata_2021-01.parquet\", include_label=False)\n",
    "\n",
    "batch_predictor.predict_pipelined(ds, blocks_per_window=10)\n",
    "# end::ml_pipeline_inference[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4087d98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::training_func_setup[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ray.data import from_torch\n",
    "\n",
    "num_samples = 20\n",
    "input_size = 10\n",
    "layer_size = 15\n",
    "output_size = 5\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_data():\n",
    "    return torch.randn(num_samples, input_size)  # <1>\n",
    "\n",
    "\n",
    "input_data = train_data()\n",
    "label_data = torch.randn(num_samples, output_size)\n",
    "train_dataset = from_torch(input_data)  # <2>\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loss_fn, optimizer):  # <3>\n",
    "    output = model(input_data)\n",
    "    loss = loss_fn(output, label_data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def training_loop():  # <4>\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "# end::training_func_setup[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf11c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::dist_func_setup[]\n",
    "from ray.train.torch import prepare_model\n",
    "\n",
    "\n",
    "def distributed_training_loop():\n",
    "    model = NeuralNetwork()\n",
    "    model = prepare_model(model)  # <1>\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "# end::dist_func_setup[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc5a35",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::train_start[]\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=distributed_training_loop,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False\n",
    "    ),\n",
    "    datasets={\"train\": train_dataset}\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "# end::train_start[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c049af7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ray_tune_integration_1[]\n",
    "import ray\n",
    "\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray import tune\n",
    "from ray.data.preprocessors import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "dataset = ray.data.from_items(\n",
    "    [{\"X\": x, \"Y\": 1} for x in range(0, 100)] +\n",
    "    [{\"X\": x, \"Y\": 0} for x in range(100, 200)]\n",
    ")\n",
    "prep_v1 = StandardScaler(columns=[\"X\"])\n",
    "prep_v2 = MinMaxScaler(columns=[\"X\"])\n",
    "\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers=tune.grid_search([2, 4]),\n",
    "        resources_per_worker={\n",
    "            \"CPU\": 2,\n",
    "            \"GPU\": 0,\n",
    "        },\n",
    "    ),\n",
    "    \"preprocessor\": tune.grid_search([prep_v1, prep_v2]),\n",
    "    \"params\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"tree_method\": \"hist\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"max_depth\": tune.randint(1, 9),\n",
    "    },\n",
    "}\n",
    "# end::ray_tune_integration_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e305d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_tune_integration_2[]\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune import Tuner\n",
    "\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    params={},\n",
    "    run_config=RunConfig(verbose=2),\n",
    "    preprocessor=None,\n",
    "    scaling_config=None,\n",
    "    label_column=\"Y\",\n",
    "    datasets={\"train\": dataset}\n",
    ")\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "# end::ray_tune_integration_2[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
