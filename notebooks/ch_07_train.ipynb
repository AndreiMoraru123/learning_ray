{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a483e38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f866504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_preprocess[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "LABEL_COLUMN = \"is_big_tip\"\n",
    "FEATURE_COLUMNS = [\"passenger_count\", \"trip_distance\", \"fare_amount\",\n",
    "                   \"trip_duration\", \"hour\", \"day_of_week\"]\n",
    "\n",
    "enable_dask_on_ray()\n",
    "\n",
    "\n",
    "def load_dataset(path: str, *, include_label=True):\n",
    "    columns = [\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\",\n",
    "               \"passenger_count\", \"trip_distance\", \"fare_amount\"]\n",
    "    df = dd.read_parquet(path, columns=columns)  # <1>\n",
    "\n",
    "    df = df.dropna()  # <2>\n",
    "    df = df[(df[\"passenger_count\"] <= 4) &\n",
    "            (df[\"trip_distance\"] < 100) &\n",
    "            (df[\"fare_amount\"] < 1000)]\n",
    "\n",
    "    df[\"tpep_pickup_datetime\"] = dd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"tpep_dropoff_datetime\"] = dd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "\n",
    "    df[\"trip_duration\"] = (df[\"tpep_dropoff_datetime\"] -\n",
    "                           df[\"tpep_pickup_datetime\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] < 4 * 60 * 60] # 4 hours.\n",
    "    df[\"hour\"] = df[\"tpep_pickup_datetime\"].dt.hour\n",
    "    df[\"day_of_week\"] = df[\"tpep_pickup_datetime\"].dt.weekday  # <3>\n",
    "\n",
    "    if include_label:\n",
    "        df[LABEL_COLUMN] = df[\"tip_amount\"] > 0.2 * df[\"fare_amount\"]  # <4>\n",
    "\n",
    "    df = df.drop(  # <5>\n",
    "        columns=[\"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"tip_amount\"]\n",
    "    )\n",
    "\n",
    "    return ray.data.from_dask(df).repartition(100)  # <6>\n",
    "# end::ml_pipeline_preprocess[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9810d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_model[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FarePredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(6, 256)\n",
    "        self.fc2 = nn.Linear(256, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "\n",
    "        return x\n",
    "# end::ml_pipeline_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8bde8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_1[]\n",
    "from ray.air import session\n",
    "from ray.air.config import ScalingConfig\n",
    "import ray.train as train\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "\n",
    "\n",
    "def train_loop_per_worker(config: dict):  # <1>\n",
    "    batch_size = config.get(\"batch_size\", 32)\n",
    "    lr = config.get(\"lr\", 1e-2)\n",
    "    num_epochs = config.get(\"num_epochs\", 3)\n",
    "\n",
    "    dataset_shard = train.get_dataset_shard(\"train\")  # <2>\n",
    "\n",
    "    model = FarePredictor()\n",
    "    dist_model = train.torch.prepare_model(model)  # <3>\n",
    "\n",
    "    loss_function = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(dist_model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):  # <4>\n",
    "        loss = 0\n",
    "        num_batches = 0\n",
    "        for batch in dataset_shard.iter_torch_batches(  # <5>\n",
    "                batch_size=batch_size, dtypes=torch.float\n",
    "        ):\n",
    "            labels = torch.unsqueeze(batch[LABEL_COLUMN], dim=1)\n",
    "            inputs = torch.cat(\n",
    "                [torch.unsqueeze(batch[f], dim=1) for f in FEATURE_COLUMNS], dim=1\n",
    "            )\n",
    "            output = dist_model(inputs)\n",
    "            batch_loss = loss_function(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            num_batches += 1\n",
    "            loss += batch_loss.item()\n",
    "\n",
    "        session.report(  # <6>\n",
    "            {\"epoch\": epoch, \"loss\": loss},\n",
    "            checkpoint=TorchCheckpoint.from_model(dist_model)\n",
    "        )\n",
    "# end::ml_pipeline_train_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e41a61",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_train_2[]\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,  # <1>\n",
    "    train_loop_config={  # <2>\n",
    "        \"lr\": 1e-2, \"num_epochs\": 3, \"batch_size\": 64\n",
    "    },\n",
    "    scaling_config=ScalingConfig(num_workers=2),  # <3>\n",
    "    datasets={  # <4>\n",
    "        \"train\": load_dataset(\"nyc_tlc_data/yellow_tripdata_2020-01.parquet\")\n",
    "    },\n",
    ")\n",
    "\n",
    "result = trainer.fit()  # <5>\n",
    "trained_model = result.checkpoint\n",
    "# end::ml_pipeline_train_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c1d8f4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ml_pipeline_inference[]\n",
    "from ray.train.torch import TorchPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "\n",
    "batch_predictor = BatchPredictor(trained_model, TorchPredictor)\n",
    "ds = load_dataset(\n",
    "    \"nyc_tlc_data/yellow_tripdata_2021-01.parquet\", include_label=False)\n",
    "\n",
    "batch_predictor.predict_pipelined(ds, blocks_per_window=10)\n",
    "# end::ml_pipeline_inference[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a04bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::training_func_setup[]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ray.data import read_datasource, datasource\n",
    "\n",
    "num_samples = 20\n",
    "input_size = 10\n",
    "layer_size = 15\n",
    "output_size = 5\n",
    "num_epochs = 3\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, layer_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(layer_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.f12(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_data():\n",
    "    return torch.randn(num_samples, input_size)  # <1>\n",
    "\n",
    "\n",
    "input_data = train_data()\n",
    "label_data = torch.randn(num_samples, output_size)\n",
    "\n",
    "source = datasource.SimpleTorchDatasource()  # <2>\n",
    "train_dataset = read_datasource(source, dataset_factory=train_data)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loss_fn, optimizer):  # <3>\n",
    "    output = model(input_data)\n",
    "    loss = loss_fn(output, label_data)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "def training_loop():  # <4>\n",
    "    model = NeuralNetwork()\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "# end::training_func_setup[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b51597",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# tag::dist_func_setup[]\n",
    "from ray.train.torch import prepare_model\n",
    "\n",
    "\n",
    "def distributed_training_loop():\n",
    "    model = NeuralNetwork()\n",
    "    model = prepare_model(model)  # <1>\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, loss_fn, optimizer)\n",
    "\n",
    "# end::dist_func_setup[]\n",
    "\n",
    "\n",
    "# TODO: this code previously didn't work. we need to provide a \"dataset\" argument here.\n",
    "# Notet that this is also wrong in the current docs, which state that this is possible.\n",
    "# TODO running this results in: Result dict has no key: training_iteration. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['trial_id', 'experiment_id', 'date', 'timestamp', 'pid', 'hostname', 'node_ip', 'config', 'done']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9e86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::train_start[]\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=distributed_training_loop,\n",
    "    scaling_config=ScalingConfig(\n",
    "        num_workers=2,\n",
    "        use_gpu=False\n",
    "    ),\n",
    "    datasets={\"train\": train_dataset}\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "# end::train_start[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ebbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_tune_integration_1[]\n",
    "from ray import tune\n",
    "from ray.data.preprocessors import StandardScaler, MinMaxScaler\n",
    "\n",
    "\n",
    "dataset = ray.data.from_items(\n",
    "    [{\"X\": 1.0, \"Y\": 2.0}, {\"X\": 4.0, \"Y\": 0.0}]\n",
    ")\n",
    "prep_v1 = StandardScaler(columns=[\"X\", \"Y\"])\n",
    "prep_v2 = MinMaxScaler(columns=[\"X\", \"Y\"])\n",
    "\n",
    "param_space = {\n",
    "    \"scaling_config\": ScalingConfig(\n",
    "        num_workers=tune.grid_search([2, 4]),\n",
    "        resources_per_worker={\n",
    "            \"CPU\": 2,\n",
    "            \"GPU\": 0,\n",
    "        },\n",
    "    ),\n",
    "    \"preprocessor\": tune.grid_search([prep_v1, prep_v2]),\n",
    "    \"params\": {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"tree_method\": \"approx\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"],\n",
    "        \"eta\": tune.loguniform(1e-4, 1e-1),\n",
    "        \"subsample\": tune.uniform(0.5, 1.0),\n",
    "        \"max_depth\": tune.randint(1, 9),\n",
    "    },\n",
    "}\n",
    "# end::ray_tune_integration_1[]\n",
    "\n",
    "# tag::ray_tune_integration_2[]\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import RunConfig\n",
    "from ray.tune import Tuner\n",
    "\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    run_config=RunConfig(verbose=1),\n",
    "    preprocessor=None,\n",
    "    scaling_config=None,\n",
    "    label_column=\"Y\",\n",
    "    datasets={\"train\": dataset}\n",
    ")\n",
    "\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space=param_space,\n",
    ")\n",
    "\n",
    "results = tuner.fit()\n",
    "# end::ray_tune_integration_2[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
