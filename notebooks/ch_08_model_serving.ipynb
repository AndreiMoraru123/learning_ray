{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f9f25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::basic_deployment[]\n",
    "from ray import serve\n",
    "\n",
    "from starlette.requests import Request\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    def __call__(self, request: Request) -> str:\n",
    "        input_text = request.query_params[\"input_text\"]\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "# end::basic_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942ba8b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::basic_deployment_bind[]\n",
    "basic_deployment = SentimentAnalysis.bind()\n",
    "# end::basic_deployment_bind[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b760a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::fastapi_deployment[]\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def classify(self, input_text: str) -> str:\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "\n",
    "\n",
    "fastapi_deployment = SentimentAnalysis.bind()\n",
    "# end::fastapi_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b549675d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::scaled_deployment[]\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 2})\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def classify(self, input_text: str) -> str:\n",
    "        import os\n",
    "        print(\"from process:\", os.getpid())\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "\n",
    "\n",
    "scaled_deployment = SentimentAnalysis.bind()\n",
    "# end::scaled_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152dbc8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::batched_deployment[]\n",
    "from typing import List\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n",
    "    async def classify_batched(self, batched_inputs: List[str]) -> List[str]:\n",
    "        print(\"Got batch size:\", len(batched_inputs))\n",
    "        results = self._classifier(batched_inputs)\n",
    "        return [result[\"label\"] for result in results]\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def classify(self, input_text: str) -> str:\n",
    "        return await self.classify_batched(input_text)\n",
    "\n",
    "\n",
    "batched_deployment = SentimentAnalysis.bind()\n",
    "# end::batched_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180092a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::multi_deployment_basic[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __call__(self, inp: str):\n",
    "        return \"Hi from downstream model!\"\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Driver:\n",
    "    def __init__(self, downstream):\n",
    "        self._d = downstream\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        return await self._d.remote()\n",
    "\n",
    "\n",
    "downstream = DownstreamModel.bind()\n",
    "driver = Driver.bind(downstream)\n",
    "# end::multi_deployment_basic[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea75381d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::pipeline_deployment[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self, inp: str):\n",
    "        return inp + \"|\" + self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class PipelineDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        intermediate = self._m1.remote(\"input\")\n",
    "        final = self._m2.remote(intermediate)\n",
    "        return await final\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "pipeline_driver = PipelineDriver.bind(m1, m2)\n",
    "# end::pipeline_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb9b08",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::broadcast_deployment[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class BroadcastDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        output1, output2 = self._m1.remote(), self._m2.remote()\n",
    "        return [await output1, await output2]\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "broadcast_driver = BroadcastDriver.bind(m1, m2)\n",
    "# end::broadcast_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a83c3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::conditional_deployment[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class ConditionalDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        import random\n",
    "        if random.random() > 0.5:\n",
    "            return await self._m1.remote()\n",
    "        else:\n",
    "            return await self._m2.remote()\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "conditional_driver = ConditionalDriver.bind(m1, m2)\n",
    "# end::conditional_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51afb41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::fetch_wikipedia[]\n",
    "from typing import Optional\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "\n",
    "def fetch_wikipedia_page(search_term: str) -> Optional[str]:\n",
    "    results = wikipedia.search(search_term)\n",
    "    # If no results, return to caller.\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the page for the top result.\n",
    "    return wikipedia.page(results[0]).content\n",
    "# end::fetch_wikipedia[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71020fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::sentiment_analysis[]\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n",
    "    async def is_positive_batched(self, inputs: List[str]) -> List[bool]:\n",
    "        results = self._classifier(inputs, truncation=True)\n",
    "        return [result[\"label\"] == \"POSITIVE\" for result in results]\n",
    "\n",
    "    async def __call__(self, input_text: str) -> bool:\n",
    "        return await self.is_positive_batched(input_text)\n",
    "# end::sentiment_analysis[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f231f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::summarizer[]\n",
    "@serve.deployment(num_replicas=2)\n",
    "class Summarizer:\n",
    "    def __init__(self, max_length: Optional[int] = None):\n",
    "        self._summarizer = pipeline(\"summarization\")\n",
    "        self._max_length = max_length\n",
    "\n",
    "    def __call__(self, input_text: str) -> str:\n",
    "        result = self._summarizer(\n",
    "            input_text, max_length=self._max_length, truncation=True)\n",
    "        return result[0][\"summary_text\"]\n",
    "# end::summarizer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d58096b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::entity_recognition[]\n",
    "@serve.deployment\n",
    "class EntityRecognition:\n",
    "    def __init__(self, threshold: float = 0.90, max_entities: int = 10):\n",
    "        self._entity_recognition = pipeline(\"ner\")\n",
    "        self._threshold = threshold\n",
    "        self._max_entities = max_entities\n",
    "\n",
    "    def __call__(self, input_text: str) -> List[str]:\n",
    "        final_results = []\n",
    "        for result in self._entity_recognition(input_text):\n",
    "            if result[\"score\"] > self._threshold:\n",
    "                final_results.append(result[\"word\"])\n",
    "            if len(final_results) == self._max_entities:\n",
    "                break\n",
    "\n",
    "        return final_results\n",
    "# end::entity_recognition[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e71705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::response_model[]\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    success: bool\n",
    "    message: str = \"\"\n",
    "    summary: str = \"\"\n",
    "    named_entities: List[str] = []\n",
    "# end::response_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1621520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::final_driver[]\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class NLPPipelineDriver:\n",
    "    def __init__(self, sentiment_analysis, summarizer, entity_recognition):\n",
    "        self._sentiment_analysis = sentiment_analysis\n",
    "        self._summarizer = summarizer\n",
    "        self._entity_recognition = entity_recognition\n",
    "\n",
    "    @app.get(\"/\", response_model=Response)\n",
    "    async def summarize_article(self, search_term: str) -> Response:\n",
    "        # Fetch the top page content for the search term if found.\n",
    "        page_content = fetch_wikipedia_page(search_term)\n",
    "        if page_content is None:\n",
    "            return Response(success=False, message=\"No pages found.\")\n",
    "\n",
    "        # Conditionally continue based on the sentiment analysis.\n",
    "        is_positive = await self._sentiment_analysis.remote(page_content)\n",
    "        if not is_positive:\n",
    "            return Response(success=False, message=\"Only positivitiy allowed!\")\n",
    "\n",
    "        # Query the summarizer and named entity recognition models in parallel.\n",
    "        summary_result = self._summarizer.remote(page_content)\n",
    "        entities_result = self._entity_recognition.remote(page_content)\n",
    "        return Response(\n",
    "            success=True,\n",
    "            summary=await summary_result,\n",
    "            named_entities=await entities_result\n",
    "        )\n",
    "# end::final_driver[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe0750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::final_pipeline[]\n",
    "sentiment_analysis = SentimentAnalysis.bind()\n",
    "summarizer = Summarizer.bind()\n",
    "entity_recognition = EntityRecognition.bind(threshold=0.95, max_entities=5)\n",
    "nlp_pipeline_driver = NLPPipelineDriver.bind(\n",
    "    sentiment_analysis, summarizer, entity_recognition)\n",
    "# end::final_pipeline[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
