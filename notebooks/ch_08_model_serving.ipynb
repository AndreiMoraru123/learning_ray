{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec4a983",
   "metadata": {},
   "source": [
    "# Chapter 8: Online Inference with Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0644db",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_08_model_serving.ipynb).\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5437fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cbc43c",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f3437",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[serve]\"==2.2.0 transformers==4.21.2 requests==2.28.1\n",
    "! pip install transformers==4.21.2 requests==2.28.1 wikipedia==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b30cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::basic_deployment[]\n",
    "from ray import serve\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    def __call__(self, request) -> str:\n",
    "        input_text = request.query_params[\"input_text\"]\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "# end::basic_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::basic_deployment_bind[]\n",
    "basic_deployment = SentimentAnalysis.bind()\n",
    "# end::basic_deployment_bind[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85158762",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::basic_deployment_query_1[]\n",
    "import requests\n",
    "\n",
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"input_text\": \"Hello friend!\"}\n",
    ").json())\n",
    "# end::basic_deployment_query_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da797f8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::fastapi_deployment[]\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def classify(self, input_text: str) -> str:\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "\n",
    "\n",
    "fastapi_deployment = SentimentAnalysis.bind()\n",
    "# end::fastapi_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8d398a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::scaled_deployment[]\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 2})\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def classify(self, input_text: str) -> str:\n",
    "        import os\n",
    "        print(\"from process:\", os.getpid())\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "\n",
    "\n",
    "scaled_deployment = SentimentAnalysis.bind()\n",
    "# end::scaled_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56166ab",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::batched_deployment[]\n",
    "from typing import List\n",
    "\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n",
    "    async def classify_batched(self, batched_inputs: List[str]) -> List[str]:\n",
    "        print(\"Got batch size:\", len(batched_inputs))\n",
    "        results = self._classifier(batched_inputs)\n",
    "        return [result[\"label\"] for result in results]\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def classify(self, input_text: str) -> str:\n",
    "        return await self.classify_batched(input_text)\n",
    "\n",
    "\n",
    "batched_deployment = SentimentAnalysis.bind()\n",
    "# end::batched_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb83ae",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::multi_deployment_basic[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __call__(self, inp: str):\n",
    "        return \"Hi from downstream model!\"\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Driver:\n",
    "    def __init__(self, downstream):\n",
    "        self._d = downstream\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        return await self._d.remote()\n",
    "\n",
    "\n",
    "downstream = DownstreamModel.bind()\n",
    "driver = Driver.bind(downstream)\n",
    "# end::multi_deployment_basic[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f89732b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::pipeline_deployment[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self, inp: str):\n",
    "        return inp + \"|\" + self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class PipelineDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        intermediate = self._m1.remote(\"input\")\n",
    "        final = self._m2.remote(intermediate)\n",
    "        return await final\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "pipeline_driver = PipelineDriver.bind(m1, m2)\n",
    "# end::pipeline_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db86732",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::broadcast_deployment[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class BroadcastDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        output1, output2 = self._m1.remote(), self._m2.remote()\n",
    "        return [await output1, await output2]\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "broadcast_driver = BroadcastDriver.bind(m1, m2)\n",
    "# end::broadcast_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63982f78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::conditional_deployment[]\n",
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class ConditionalDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        import random\n",
    "        if random.random() > 0.5:\n",
    "            return await self._m1.remote()\n",
    "        else:\n",
    "            return await self._m2.remote()\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "conditional_driver = ConditionalDriver.bind(m1, m2)\n",
    "# end::conditional_deployment[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21538fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::fetch_wikipedia[]\n",
    "from typing import Optional\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "\n",
    "def fetch_wikipedia_page(search_term: str) -> Optional[str]:\n",
    "    results = wikipedia.search(search_term)\n",
    "    # If no results, return to caller.\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the page for the top result.\n",
    "    return wikipedia.page(results[0]).content\n",
    "# end::fetch_wikipedia[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a938210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::sentiment_analysis[]\n",
    "from ray import serve\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n",
    "    async def is_positive_batched(self, inputs: List[str]) -> List[bool]:\n",
    "        results = self._classifier(inputs, truncation=True)\n",
    "        return [result[\"label\"] == \"POSITIVE\" for result in results]\n",
    "\n",
    "    async def __call__(self, input_text: str) -> bool:\n",
    "        return await self.is_positive_batched(input_text)\n",
    "# end::sentiment_analysis[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::summarizer[]\n",
    "@serve.deployment(num_replicas=2)\n",
    "class Summarizer:\n",
    "    def __init__(self, max_length: Optional[int] = None):\n",
    "        self._summarizer = pipeline(\"summarization\")\n",
    "        self._max_length = max_length\n",
    "\n",
    "    def __call__(self, input_text: str) -> str:\n",
    "        result = self._summarizer(\n",
    "            input_text, max_length=self._max_length, truncation=True)\n",
    "        return result[0][\"summary_text\"]\n",
    "# end::summarizer[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c61fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::entity_recognition[]\n",
    "@serve.deployment\n",
    "class EntityRecognition:\n",
    "    def __init__(self, threshold: float = 0.90, max_entities: int = 10):\n",
    "        self._entity_recognition = pipeline(\"ner\")\n",
    "        self._threshold = threshold\n",
    "        self._max_entities = max_entities\n",
    "\n",
    "    def __call__(self, input_text: str) -> List[str]:\n",
    "        final_results = []\n",
    "        for result in self._entity_recognition(input_text):\n",
    "            if result[\"score\"] > self._threshold:\n",
    "                final_results.append(result[\"word\"])\n",
    "            if len(final_results) == self._max_entities:\n",
    "                break\n",
    "\n",
    "        return final_results\n",
    "# end::entity_recognition[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e969ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::response_model[]\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    success: bool\n",
    "    message: str = \"\"\n",
    "    summary: str = \"\"\n",
    "    named_entities: List[str] = []\n",
    "# end::response_model[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::final_driver[]\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class NLPPipelineDriver:\n",
    "    def __init__(self, sentiment_analysis, summarizer, entity_recognition):\n",
    "        self._sentiment_analysis = sentiment_analysis\n",
    "        self._summarizer = summarizer\n",
    "        self._entity_recognition = entity_recognition\n",
    "\n",
    "    @app.get(\"/\", response_model=Response)\n",
    "    async def summarize_article(self, search_term: str) -> Response:\n",
    "        # Fetch the top page content for the search term if found.\n",
    "        page_content = fetch_wikipedia_page(search_term)\n",
    "        if page_content is None:\n",
    "            return Response(success=False, message=\"No pages found.\")\n",
    "\n",
    "        # Conditionally continue based on the sentiment analysis.\n",
    "        is_positive = await self._sentiment_analysis.remote(page_content)\n",
    "        if not is_positive:\n",
    "            return Response(success=False, message=\"Only positivitiy allowed!\")\n",
    "\n",
    "        # Query the summarizer and named entity recognition models in parallel.\n",
    "        summary_result = self._summarizer.remote(page_content)\n",
    "        entities_result = self._entity_recognition.remote(page_content)\n",
    "        return Response(\n",
    "            success=True,\n",
    "            summary=await summary_result,\n",
    "            named_entities=await entities_result\n",
    "        )\n",
    "# end::final_driver[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b807836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::final_pipeline[]\n",
    "sentiment_analysis = SentimentAnalysis.bind()\n",
    "summarizer = Summarizer.bind()\n",
    "entity_recognition = EntityRecognition.bind(threshold=0.95, max_entities=5)\n",
    "nlp_pipeline_driver = NLPPipelineDriver.bind(\n",
    "    sentiment_analysis, summarizer, entity_recognition)\n",
    "# end::final_pipeline[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8efe5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# tag::basic_deployment_query_2[]\n",
    "import requests\n",
    "\n",
    "\n",
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"search_term\": \"rayserve\"}\n",
    ").text)\n",
    "# end::basic_deployment_query_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4135d7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::basic_deployment_query_3[]\n",
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"search_term\": \"war\"}\n",
    ").text)\n",
    "# end::basic_deployment_query_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e73f3a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::basic_deployment_query_4[]\n",
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"search_term\": \"physicist\"}\n",
    ").text)\n",
    "# end::basic_deployment_query_4[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
