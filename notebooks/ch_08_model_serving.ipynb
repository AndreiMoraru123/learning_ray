{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d84a44",
   "metadata": {},
   "source": [
    "# Chapter 8: Online Inference with Ray Serve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc579d9",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_08_model_serving.ipynb).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_08_model_serving.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19924c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1545f53d",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9633bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mRequirement already satisfied: ray[serve]>=2.1.0 in /usr/local/lib/python3.9/site-packages (2.1.0)\r\n",
      "Collecting transformers==4.21.2\r\n",
      "  Using cached transformers-4.21.2-py3-none-any.whl (4.7 MB)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (5.4.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (0.12.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (3.3.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (21.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (2021.4.4)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (4.42.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (1.23.3)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (2.25.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/site-packages (from transformers==4.21.2) (0.5.1)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.0.3)\r\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.2.0)\r\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (20.3.0)\r\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (3.2.0)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (3.19.6)\r\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (7.1.2)\r\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.3.0)\r\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (20.8.1)\r\n",
      "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.43.0)\r\n",
      "Requirement already satisfied: colorful in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.5.4)\r\n",
      "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (3.8.1)\r\n",
      "Requirement already satisfied: smart-open in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (6.2.0)\r\n",
      "Requirement already satisfied: aiorwlock in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.3.0)\r\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.75.2)\r\n",
      "Requirement already satisfied: aiohttp-cors in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.7.0)\r\n",
      "Requirement already satisfied: gpustat>=1.0.0 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.0.0)\r\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.16.0)\r\n",
      "Requirement already satisfied: starlette in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.17.1)\r\n",
      "Requirement already satisfied: opencensus in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.11.0)\r\n",
      "Requirement already satisfied: prometheus-client<0.14.0,>=0.7.1 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.12.0)\r\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (1.9.0)\r\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /usr/local/lib/python3.9/site-packages (from ray[serve]>=2.1.0) (0.3.14)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]>=2.1.0) (1.7.2)\r\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]>=2.1.0) (2.0.12)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]>=2.1.0) (4.0.2)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.9/site-packages (from aiohttp>=3.7->ray[serve]>=2.1.0) (6.0.2)\r\n",
      "Requirement already satisfied: blessed>=1.17.1 in /usr/local/lib/python3.9/site-packages (from gpustat>=1.0.0->ray[serve]>=2.1.0) (1.17.6)\r\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.9/site-packages/six-1.15.0-py3.9.egg (from gpustat>=1.0.0->ray[serve]>=2.1.0) (1.15.0)\r\n",
      "Requirement already satisfied: nvidia-ml-py<=11.495.46,>=11.450.129 in /usr/local/lib/python3.9/site-packages (from gpustat>=1.0.0->ray[serve]>=2.1.0) (11.495.46)\r\n",
      "Requirement already satisfied: psutil>=5.6.0 in /usr/local/lib/python3.9/site-packages (from gpustat>=1.0.0->ray[serve]>=2.1.0) (5.9.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.21.2) (3.10.0.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers==4.21.2) (2.4.7)\r\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /usr/local/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[serve]>=2.1.0) (2.4.0)\r\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /usr/local/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[serve]>=2.1.0) (0.3.3)\r\n",
      "Requirement already satisfied: backports.entry-points-selectable>=1.0.4 in /usr/local/lib/python3.9/site-packages (from virtualenv>=20.0.24->ray[serve]>=2.1.0) (1.1.0)\r\n",
      "Requirement already satisfied: anyio<4,>=3.0.0 in /usr/local/lib/python3.9/site-packages (from starlette->ray[serve]>=2.1.0) (3.5.0)\r\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.9/site-packages (from jsonschema->ray[serve]>=2.1.0) (0.17.3)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from jsonschema->ray[serve]>=2.1.0) (59.5.0)\r\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /usr/local/lib/python3.9/site-packages (from opencensus->ray[serve]>=2.1.0) (0.1.3)\r\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.9/site-packages (from opencensus->ray[serve]>=2.1.0) (2.10.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.21.2) (2020.12.5)\r\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.21.2) (4.0.0)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.21.2) (2.10)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers==4.21.2) (1.26.12)\r\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.9/site-packages (from uvicorn->ray[serve]>=2.1.0) (0.13.0)\r\n",
      "Requirement already satisfied: asgiref>=3.4.0 in /usr/local/lib/python3.9/site-packages (from uvicorn->ray[serve]>=2.1.0) (3.5.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.9/site-packages (from anyio<4,>=3.0.0->starlette->ray[serve]>=2.1.0) (1.2.0)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.9/site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[serve]>=2.1.0) (0.2.5)\r\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]>=2.1.0) (2.11.0)\r\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]>=2.1.0) (1.56.4)\r\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\r\n",
      "  Using cached protobuf-4.21.9-cp37-abi3-macosx_10_9_universal2.whl (483 kB)\r\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]>=2.1.0) (4.2.1)\r\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]>=2.1.0) (0.2.8)\r\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]>=2.1.0) (4.9)\r\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[serve]>=2.1.0) (0.4.8)\r\n",
      "Installing collected packages: protobuf, transformers\r\n",
      "  Attempting uninstall: protobuf\r\n",
      "    Found existing installation: protobuf 3.19.6\r\n",
      "    Uninstalling protobuf-3.19.6:\r\n",
      "      Successfully uninstalled protobuf-3.19.6\r\n",
      "\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.18.0\r\n",
      "    Uninstalling transformers-4.18.0:\r\n",
      "      Successfully uninstalled transformers-4.18.0\r\n",
      "\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.9 which is incompatible.\r\n",
      "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 4.21.9 which is incompatible.\r\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.9 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed protobuf-4.21.9 transformers-4.21.2\r\n",
      "\u001B[33mWARNING: There was an error checking the latest version of pip.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0mCollecting requests==2.28.1\r\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\r\n",
      "Collecting wikipedia==1.4.0\r\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\r\n",
      "  Preparing metadata (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests==2.28.1) (2.10)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests==2.28.1) (2020.12.5)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests==2.28.1) (1.26.12)\r\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/site-packages (from requests==2.28.1) (2.0.12)\r\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/site-packages (from wikipedia==1.4.0) (4.9.3)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.2)\r\n",
      "Building wheels for collected packages: wikipedia\r\n",
      "  Building wheel for wikipedia (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=075c09ebaba55311c55a902b9d1df01101a9c1a4c8327dd8d9807ce1888e5a7b\r\n",
      "  Stored in directory: /Users/maxpumperla/Library/Caches/pip/wheels/c2/46/f4/caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\r\n",
      "Successfully built wikipedia\r\n",
      "Installing collected packages: requests, wikipedia\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.25.1\r\n",
      "    Uninstalling requests-2.25.1:\r\n",
      "      Successfully uninstalled requests-2.25.1\r\n",
      "\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001B[0m\u001B[33m\r\n",
      "\u001B[0m\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorboard 2.10.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.21.9 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed requests-2.28.1 wikipedia-1.4.0\r\n",
      "\u001B[33mWARNING: There was an error checking the latest version of pip.\u001B[0m\u001B[33m\r\n",
      "\u001B[0m"
     ]
    }
   ],
   "source": [
    "! pip install \"ray[serve]>=2.1.0\" \"transformers==4.21.2\"\n",
    "! pip install \"requests==2.28.1\" \"wikipedia==1.4.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2fed7",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fe8c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18b1864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 14:12:30.401697: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/y_/l41py1sx7bl5n30jygrsv0q40000gn/T/ipykernel_14502/2959941426.py\u001B[0m in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mray\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mserve\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpipeline\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;31m# Check the dependencies satisfy the minimal versions required.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdependency_versions_check\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m from .utils import (\n\u001B[1;32m     32\u001B[0m     \u001B[0mOptionalDependencyNotAvailable\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/dependency_versions_check.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mdependency_versions_table\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdeps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrequire_version\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequire_version_core\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/utils/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     44\u001B[0m     \u001B[0mto_py_obj\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m )\n\u001B[0;32m---> 46\u001B[0;31m from .hub import (\n\u001B[0m\u001B[1;32m     47\u001B[0m     \u001B[0mCLOUDFRONT_DISTRIB_PREFIX\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0mDISABLE_TELEMETRY\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/transformers/utils/hub.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mfilelock\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mFileLock\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mhuggingface_hub\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mHfFolder\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mRepository\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcreate_repo\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlist_repo_files\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwhoami\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexceptions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mHTTPError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mrequests\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodels\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mResponse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/huggingface_hub/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mhub_mixin\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mModelHubMixin\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPyTorchModelHubMixin\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0minference_api\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mInferenceApi\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m from .keras_mixin import (\n\u001B[0m\u001B[1;32m     64\u001B[0m     \u001B[0mKerasModelHubMixin\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0mfrom_pretrained_keras\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/huggingface_hub/keras_mixin.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     22\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0mis_tf_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 24\u001B[0;31m     \u001B[0;32mimport\u001B[0m \u001B[0mtensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     25\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtyping\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_typing\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtools\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmodule_util\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_module_util\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutil\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlazy_loader\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLazyLoader\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_LazyLoader\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpywrap_tensorflow\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0m_pywrap_tensorflow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0meager\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcontext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[0;31m# pylint: enable=wildcard-import\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/python/eager/context.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msix\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfunction_pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mconfig_pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     31\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotobuf\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcoordination_config_pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/core/framework/function_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mattr_value_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_attr__value__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnode_def_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_node__def__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mop_def_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_op__def__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/core/framework/attr_value_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_shape_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtypes_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/core/framework/tensor_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mresource_handle_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_resource__handle__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_shape_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtypes_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/core/framework/resource_handle_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtensor_shape_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mtensorflow\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mframework\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtypes_pb2\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtensorflow_dot_core_dot_framework_dot_types__pb2\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/tensorflow/core/framework/tensor_shape_pb2.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     34\u001B[0m   \u001B[0mcontaining_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m   fields=[\n\u001B[0;32m---> 36\u001B[0;31m     _descriptor.FieldDescriptor(\n\u001B[0m\u001B[1;32m     37\u001B[0m       \u001B[0mname\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'size'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfull_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'tensorflow.TensorShapeProto.Dim.size'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mindex\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m       \u001B[0mnumber\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcpp_type\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlabel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/google/protobuf/descriptor.py\u001B[0m in \u001B[0;36m__new__\u001B[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001B[0m\n\u001B[1;32m    558\u001B[0m                 \u001B[0mhas_default_value\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcontaining_oneof\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mjson_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    559\u001B[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001B[0;32m--> 560\u001B[0;31m       \u001B[0m_message\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMessage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_CheckCalledFromGeneratedFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    561\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mis_extension\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0m_message\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdefault_pool\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFindExtensionByName\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfull_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Descriptors cannot not be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "from ray import serve\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    def __call__(self, request) -> str:\n",
    "        input_text = request.query_params[\"input_text\"]\n",
    "        return self._classifier(input_text)[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60908300",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_deployment = SentimentAnalysis.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7659c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a separate process to avoid any blocking:\n",
    "! serve run app:basic_deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042a810c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"input_text\": \"Hello friend!\"}\n",
    ").json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8452a0b4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def classify(self, input_text: str) -> str:\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "\n",
    "\n",
    "fastapi_deployment = SentimentAnalysis.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987200e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment(num_replicas=2, ray_actor_options={\"num_cpus\": 2})\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    def classify(self, input_text: str) -> str:\n",
    "        import os\n",
    "        print(\"from process:\", os.getpid())\n",
    "        return self._classifier(input_text)[0][\"label\"]\n",
    "\n",
    "\n",
    "scaled_deployment = SentimentAnalysis.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b7996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n",
    "    async def classify_batched(self, batched_inputs):\n",
    "        print(\"Got batch size:\", len(batched_inputs))\n",
    "        results = self._classifier(batched_inputs)\n",
    "        return [result[\"label\"] for result in results]\n",
    "\n",
    "    @app.get(\"/\")\n",
    "    async def classify(self, input_text: str) -> str:\n",
    "        return await self.classify_batched(input_text)\n",
    "\n",
    "\n",
    "batched_deployment = SentimentAnalysis.bind()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167c10b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "from app import batched_deployment\n",
    "\n",
    "handle = serve.run(batched_deployment)\n",
    "ray.get([handle.classify.remote(\"sample text\") for _ in range(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb34fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __call__(self, inp: str):\n",
    "        return \"Hi from downstream model!\"\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class Driver:\n",
    "    def __init__(self, downstream):\n",
    "        self._d = downstream\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        return await self._d.remote()\n",
    "\n",
    "\n",
    "downstream = DownstreamModel.bind()\n",
    "driver = Driver.bind(downstream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ac5a3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self, inp: str):\n",
    "        return inp + \"|\" + self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class PipelineDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        intermediate = self._m1.remote(\"input\")\n",
    "        final = self._m2.remote(intermediate)\n",
    "        return await final\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "pipeline_driver = PipelineDriver.bind(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5cfea3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class BroadcastDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        output1, output2 = self._m1.remote(), self._m2.remote()\n",
    "        return [await output1, await output2]\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "broadcast_driver = BroadcastDriver.bind(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b08ba4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class DownstreamModel:\n",
    "    def __init__(self, my_val: str):\n",
    "        self._my_val = my_val\n",
    "\n",
    "    def __call__(self):\n",
    "        return self._my_val\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class ConditionalDriver:\n",
    "    def __init__(self, model1, model2):\n",
    "        self._m1 = model1\n",
    "        self._m2 = model2\n",
    "\n",
    "    async def __call__(self, *args) -> str:\n",
    "        import random\n",
    "        if random.random() > 0.5:\n",
    "            return await self._m1.remote()\n",
    "        else:\n",
    "            return await self._m2.remote()\n",
    "\n",
    "\n",
    "m1 = DownstreamModel.bind(\"val1\")\n",
    "m2 = DownstreamModel.bind(\"val2\")\n",
    "conditional_driver = ConditionalDriver.bind(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import wikipedia\n",
    "\n",
    "\n",
    "def fetch_wikipedia_page(search_term: str) -> Optional[str]:\n",
    "    results = wikipedia.search(search_term)\n",
    "    # If no results, return to caller.\n",
    "    if len(results) == 0:\n",
    "        return None\n",
    "\n",
    "    # Get the page for the top result.\n",
    "    return wikipedia.page(results[0]).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646606f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import serve\n",
    "from transformers import pipeline\n",
    "from typing import List\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "class SentimentAnalysis:\n",
    "    def __init__(self):\n",
    "        self._classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "    @serve.batch(max_batch_size=10, batch_wait_timeout_s=0.1)\n",
    "    async def is_positive_batched(self, inputs: List[str]) -> List[bool]:\n",
    "        results = self._classifier(inputs, truncation=True)\n",
    "        return [result[\"label\"] == \"POSITIVE\" for result in results]\n",
    "\n",
    "    async def __call__(self, input_text: str) -> bool:\n",
    "        return await self.is_positive_batched(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d37f206",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=2)\n",
    "class Summarizer:\n",
    "    def __init__(self, max_length: Optional[int] = None):\n",
    "        self._summarizer = pipeline(\"summarization\")\n",
    "        self._max_length = max_length\n",
    "\n",
    "    def __call__(self, input_text: str) -> str:\n",
    "        result = self._summarizer(\n",
    "            input_text, max_length=self._max_length, truncation=True)\n",
    "        return result[0][\"summary_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931685ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment\n",
    "class EntityRecognition:\n",
    "    def __init__(self, threshold: float = 0.90, max_entities: int = 10):\n",
    "        self._entity_recognition = pipeline(\"ner\")\n",
    "        self._threshold = threshold\n",
    "        self._max_entities = max_entities\n",
    "\n",
    "    def __call__(self, input_text: str) -> List[str]:\n",
    "        final_results = []\n",
    "        for result in self._entity_recognition(input_text):\n",
    "            if result[\"score\"] > self._threshold:\n",
    "                final_results.append(result[\"word\"])\n",
    "            if len(final_results) == self._max_entities:\n",
    "                break\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b925bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Response(BaseModel):\n",
    "    success: bool\n",
    "    message: str = \"\"\n",
    "    summary: str = \"\"\n",
    "    named_entities: List[str] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479767a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "\n",
    "@serve.deployment\n",
    "@serve.ingress(app)\n",
    "class NLPPipelineDriver:\n",
    "    def __init__(self, sentiment_analysis, summarizer, entity_recognition):\n",
    "        self._sentiment_analysis = sentiment_analysis\n",
    "        self._summarizer = summarizer\n",
    "        self._entity_recognition = entity_recognition\n",
    "\n",
    "    @app.get(\"/\", response_model=Response)\n",
    "    async def summarize_article(self, search_term: str) -> Response:\n",
    "        # Fetch the top page content for the search term if found.\n",
    "        page_content = fetch_wikipedia_page(search_term)\n",
    "        if page_content is None:\n",
    "            return Response(success=False, message=\"No pages found.\")\n",
    "\n",
    "        # Conditionally continue based on the sentiment analysis.\n",
    "        is_positive = await self._sentiment_analysis.remote(page_content)\n",
    "        if not is_positive:\n",
    "            return Response(success=False, message=\"Only positivitiy allowed!\")\n",
    "\n",
    "        # Query the summarizer and named entity recognition models in parallel.\n",
    "        summary_result = self._summarizer.remote(page_content)\n",
    "        entities_result = self._entity_recognition.remote(page_content)\n",
    "        return Response(\n",
    "            success=True,\n",
    "            summary=await summary_result,\n",
    "            named_entities=await entities_result\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis = SentimentAnalysis.bind()\n",
    "summarizer = Summarizer.bind()\n",
    "entity_recognition = EntityRecognition.bind(threshold=0.95, max_entities=5)\n",
    "nlp_pipeline_driver = NLPPipelineDriver.bind(\n",
    "    sentiment_analysis, summarizer, entity_recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62408257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a separate process to avoid any blocking:\n",
    "! serve run app:nlp_pipeline_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f456cfc9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"search_term\": \"rayserve\"}\n",
    ").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448177a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"search_term\": \"war\"}\n",
    ").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6dccf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(requests.get(\n",
    "    \"http://localhost:8000/\", params={\"search_term\": \"physicist\"}\n",
    ").text)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
