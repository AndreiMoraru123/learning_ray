{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d73a7710",
   "metadata": {},
   "source": [
    "# Chapter 5: Hyperparameter Optimization with Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab14506",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_05_tune.ipynb).\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43919cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72fb80",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9789fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[tune]\"\n",
    "! pip install bayesian-optimization\n",
    "! pip install hyperopt\n",
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ab798",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from maze_gym_env import Environment\n",
    "import time\n",
    "\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A Policy suggests actions based on the current state.\n",
    "        We do this by tracking the value of each state-action pair.\n",
    "        \"\"\"\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)]\n",
    "            for _ in range(env.observation_space.n)\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):\n",
    "        \"\"\"Explore randomly or exploit the best value currently available.\"\"\"\n",
    "        if explore and random.uniform(0, 1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])\n",
    "\n",
    "\n",
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Simulates rollouts of an environment, given a policy to follow.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):\n",
    "        \"\"\"Returns experiences for a policy rollout.\"\"\"\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            experiences.append([state, action, reward, next_state])\n",
    "            state = next_state\n",
    "            if render:\n",
    "                time.sleep(0.05)\n",
    "                self.env.render()\n",
    "\n",
    "        return experiences\n",
    "\n",
    "\n",
    "def update_policy(policy, experiences, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Updates a given policy with a list of (state, action, reward, state)\n",
    "    experiences.\"\"\"\n",
    "    for state, action, reward, next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = (1 - weight) * value + weight * \\\n",
    "                    (reward + discount_factor * next_max)\n",
    "        policy.state_action_table[state][action] = new_value\n",
    "\n",
    "\n",
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Training a policy by updating it with rollout experiences.\"\"\"\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)\n",
    "        update_policy(policy, experiences, weight, discount_factor)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)\n",
    "        steps += len(experiences)\n",
    "\n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "    return steps / num_episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a32b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::search_space[]\n",
    "import random\n",
    "search_space = []\n",
    "for i in range(10):\n",
    "    random_choice = {\n",
    "        'weight': random.uniform(0, 1),\n",
    "        'discount_factor': random.uniform(0, 1)\n",
    "    }\n",
    "    search_space.append(random_choice)\n",
    "# end::search_space[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::objective[]\n",
    "import ray\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def objective(config):  # <1>\n",
    "    environment = Environment()\n",
    "    policy = train_policy(  # <2>\n",
    "        environment,\n",
    "        weight=config[\"weight\"],\n",
    "        discount_factor=config[\"discount_factor\"]\n",
    "    )\n",
    "    score = evaluate_policy(environment, policy)  # <3>\n",
    "    return [score, config]  # <4>\n",
    "# end::objective[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6ded7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::random_search[]\n",
    "result_objects = [objective.remote(choice) for choice in search_space]\n",
    "results = ray.get(result_objects)\n",
    "\n",
    "results.sort(key=lambda x: x[0])\n",
    "print(results[-1])\n",
    "# end::random_search[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17a282e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_search_space[]\n",
    "from ray import tune\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"weight\": tune.uniform(0, 1),\n",
    "    \"discount_factor\": tune.uniform(0, 1),\n",
    "}\n",
    "# end::tune_search_space[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09bc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::tune_objective[]\n",
    "def tune_objective(config):\n",
    "    environment = Environment()\n",
    "    policy = train_policy(\n",
    "        environment,\n",
    "        weight=config[\"weight\"],\n",
    "        discount_factor=config[\"discount_factor\"]\n",
    "    )\n",
    "    score = evaluate_policy(environment, policy)\n",
    "\n",
    "    return {\"score\": score}\n",
    "# end::tune_objective[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f386436b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_analysis[]\n",
    "analysis = tune.run(tune_objective, config=search_space)\n",
    "print(analysis.get_best_config(metric=\"score\", mode=\"min\"))\n",
    "# end::tune_analysis[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78301f78",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_bo[]\n",
    "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
    "\n",
    "\n",
    "algo = BayesOptSearch(random_search_steps=4)\n",
    "\n",
    "tune.run(\n",
    "    tune_objective,\n",
    "    config=search_space,\n",
    "    metric=\"score\",\n",
    "    mode=\"min\",\n",
    "    search_alg=algo,\n",
    "    stop={\"training_iteration\": 10},\n",
    ")\n",
    "# end::tune_bo[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ad0b38",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::scheduler_obj[]\n",
    "def objective(config):\n",
    "    for step in range(30):  # <1>\n",
    "        score = config[\"weight\"] * (step ** 0.5) + config[\"bias\"]\n",
    "        tune.report(score=score)  # <2>\n",
    "\n",
    "\n",
    "search_space = {\"weight\": tune.uniform(0, 1), \"bias\": tune.uniform(0, 1)}\n",
    "# end::scheduler_obj[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df51e53",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_scheduler[]\n",
    "from ray.tune.schedulers import HyperBandScheduler\n",
    "\n",
    "\n",
    "scheduler = HyperBandScheduler(metric=\"score\", mode=\"min\")\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=10,\n",
    ")\n",
    "\n",
    "print(analysis.get_best_config(metric=\"score\", mode=\"min\"))\n",
    "# end::tune_scheduler[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94dc0d5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_simple_objective[]\n",
    "from ray import tune\n",
    "\n",
    "tune.run(objective, num_samples=10, resources_per_trial={\"cpu\": 2, \"gpu\": 0.5})\n",
    "# end::tune_simple_objective[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f42f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::tune_metrics_1[]\n",
    "from ray import tune\n",
    "from ray.tune import Callback\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "\n",
    "class PrintResultCallback(Callback):\n",
    "    def on_trial_result(self, iteration, trials, trial, result, **info):\n",
    "        print(f\"Trial {trial} in iteration {iteration}, \"\n",
    "              f\"got result: {result['score']}\")\n",
    "\n",
    "\n",
    "def objective(config):\n",
    "    for step in range(30):\n",
    "        score = config[\"weight\"] * (step ** 0.5) + config[\"bias\"]\n",
    "        tune.report(score=score, step=step, more_metrics={})\n",
    "# end::tune_metrics_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba7dfb6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# tag::tune_metrics_2[]\n",
    "search_space = {\"weight\": tune.uniform(0, 1), \"bias\": tune.uniform(0, 1)}\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    mode=\"min\",\n",
    "    metric=\"score\",\n",
    "    callbacks=[PrintResultCallback()])\n",
    "\n",
    "best = analysis.best_trial\n",
    "print(pretty_print(best.last_result))\n",
    "# end::tune_metrics_2[]\n",
    "\n",
    "\n",
    "# TODO: don't make this hardcoded?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef1e84f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_resume[]\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    name=\"/Users/maxpumperla/ray_results/objective_2022-05-23_15-52-01\",\n",
    "    resume=True,\n",
    "    config=search_space)\n",
    "# end::tune_resume[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8957fe4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_stop_dict[]\n",
    "tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    stop={\"training_iteration\": 10})\n",
    "# end::tune_stop_dict[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6b5d63",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_stop_function[]\n",
    "def stopper(trial_id, result):\n",
    "    return result[\"score\"] < 2\n",
    "\n",
    "\n",
    "tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    stop=stopper)\n",
    "# end::tune_stop_function[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346d4af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_custom_space[]\n",
    "from ray import tune\n",
    "import numpy as np\n",
    "\n",
    "search_space = {\n",
    "    \"weight\": tune.sample_from(\n",
    "        lambda context: np.random.uniform(low=0.0, high=1.0)\n",
    "    ),\n",
    "    \"bias\": tune.sample_from(\n",
    "        lambda context: context.config.weight * np.random.normal()\n",
    "    )}\n",
    "\n",
    "tune.run(objective, config=search_space)\n",
    "# end::tune_custom_space[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a76b33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_rllib[]\n",
    "from ray import tune\n",
    "\n",
    "analysis = tune.run(\n",
    "    \"DQN\",\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "    config={\n",
    "        \"env\": \"CartPole-v0\",\n",
    "        \"lr\": tune.uniform(1e-5, 1e-4),\n",
    "        \"train_batch_size\": tune.choice([10000, 20000, 40000]),\n",
    "    },\n",
    ")\n",
    "# end::tune_rllib[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6278b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::tune_keras_1[]\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    num_classes = 10\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "load_data()\n",
    "# end::tune_keras_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd79c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::tune_keras_2[]\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from ray.tune.integration.keras import TuneReportCallback\n",
    "\n",
    "\n",
    "def objective(config):\n",
    "    (x_train, y_train), (x_test, y_test) = load_data()\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))\n",
    "    model.add(Dense(config[\"hidden\"], activation=config[\"activation\"]))\n",
    "    model.add(Dropout(config[\"rate\"]))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    model.fit(x_train, y_train, batch_size=128, epochs=10,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks=[TuneReportCallback({\"mean_accuracy\": \"accuracy\"})])\n",
    "# end::tune_keras_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc52ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::tune_keras_3[]\n",
    "from ray import tune\n",
    "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
    "\n",
    "\n",
    "initial_params = [{\"rate\": 0.2, \"hidden\": 128, \"activation\": \"relu\"}]\n",
    "algo = HyperOptSearch(points_to_evaluate=initial_params)\n",
    "\n",
    "search_space = {\n",
    "    \"rate\": tune.uniform(0.1, 0.5),\n",
    "    \"hidden\": tune.randint(32, 512),\n",
    "    \"activation\": tune.choice([\"relu\", \"tanh\"])\n",
    "}\n",
    "\n",
    "\n",
    "analysis = tune.run(\n",
    "    objective,\n",
    "    name=\"keras_hyperopt_exp\",\n",
    "    search_alg=algo,\n",
    "    metric=\"mean_accuracy\",\n",
    "    mode=\"max\",\n",
    "    stop={\"mean_accuracy\": 0.99},\n",
    "    num_samples=10,\n",
    "    config=search_space,\n",
    ")\n",
    "print(\"Best hyperparameters found were: \", analysis.best_config)\n",
    "# end::tune_keras_3[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
