{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb6bfd0",
   "metadata": {},
   "source": [
    "# Chapter 3: Building Your First Distributed Application With Ray Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c204e",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_03_core_app.ipynb).\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2191e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb000a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::discrete_actions[]\n",
    "import random\n",
    "\n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self, num_actions: int):\n",
    "        \"\"\" Discrete action space for num_actions.\n",
    "        Discrete(4) can be used as encoding moving in\n",
    "        one of the cardinal directions.\n",
    "        \"\"\"\n",
    "        self.n = num_actions\n",
    "\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n - 1)  # <1>\n",
    "\n",
    "\n",
    "space = Discrete(4)\n",
    "print(space.sample())  # <2>\n",
    "# end::discrete_actions[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::init_env[]\n",
    "import os\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self,  *args, **kwargs):\n",
    "        self.seeker, self.goal = (0, 0), (4, 4)  # <1>\n",
    "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
    "\n",
    "        self.action_space = Discrete(4)  # <2>\n",
    "        self.observation_space = Discrete(5*5)  # <3>\n",
    "# end::init_env[]\n",
    "\n",
    "# tag::env_helpers[]\n",
    "    def reset(self):  # <1>\n",
    "        \"\"\"Reset seeker position and return observations.\"\"\"\n",
    "        self.seeker = (0, 0)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return 5 * self.seeker[0] + self.seeker[1]  # <2>\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal\"\"\"\n",
    "        return 1 if self.seeker == self.goal else 0  # <3>\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"We're done if we found the goal\"\"\"\n",
    "        return self.seeker == self.goal  # <4>\n",
    "# end::env_helpers[]\n",
    "\n",
    "# tag::env_step[]\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in a direction and return all available information.\"\"\"\n",
    "        if action == 0:  # move down\n",
    "            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\n",
    "        elif action == 1:  # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1] - 1, 0))\n",
    "        elif action == 2:  # move up\n",
    "            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\n",
    "        elif action == 3:  # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1] + 1, 4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        obs = self.get_observation()\n",
    "        rew = self.get_reward()\n",
    "        done = self.is_done()\n",
    "        return obs, rew, done, self.info  # <1>\n",
    "# end::env_step[]\n",
    "\n",
    "# tag::env_render[]\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"Render the environment, e.g. by printing its representation.\"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')  # <1>\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'  # <2>\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))  # <3>\n",
    "# end::env_render[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8dc78b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::env_test[]\n",
    "import time\n",
    "\n",
    "environment = Environment()\n",
    "\n",
    "while not environment.is_done():\n",
    "    random_action = environment.action_space.sample()  # <1>\n",
    "    environment.step(random_action)\n",
    "    time.sleep(0.1)\n",
    "    environment.render()  # <2>\n",
    "# end::env_test[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8eecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::policy[]\n",
    "import numpy as np\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A Policy suggests actions based on the current state.\n",
    "        We do this by tracking the value of each state-action pair.\n",
    "        \"\"\"\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)]\n",
    "            for _ in range(env.observation_space.n)  # <1>\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):  # <2>\n",
    "        \"\"\"Explore randomly or exploit the best value currently available.\"\"\"\n",
    "        if explore and random.uniform(0, 1) < epsilon:  # <3>\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])  # <4>\n",
    "# end::policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::simulation[]\n",
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Simulates rollouts of an environment, given a policy to follow.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):  # <1>\n",
    "        \"\"\"Returns experiences for a policy rollout.\"\"\"\n",
    "        experiences = []\n",
    "        state = self.env.reset()  # <2>\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)  # <3>\n",
    "            next_state, reward, done, info = self.env.step(action)  # <4>\n",
    "            experiences.append([state, action, reward, next_state])  # <5>\n",
    "            state = next_state\n",
    "            if render:  # <6>\n",
    "                time.sleep(0.05)\n",
    "                self.env.render()\n",
    "\n",
    "        return experiences\n",
    "# end::simulation[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503b079",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::naive_rollout[]\n",
    "untrained_policy = Policy(environment)\n",
    "sim = Simulation(environment)\n",
    "\n",
    "exp = sim.rollout(untrained_policy, render=True, epsilon=1.0)  # <1>\n",
    "for row in untrained_policy.state_action_table:\n",
    "    print(row)  # <2>\n",
    "# end::naive_rollout[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::update_policy[]\n",
    "def update_policy(policy, experiences, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Updates a given policy with a list of (state, action, reward, state)\n",
    "    experiences.\"\"\"\n",
    "    for state, action, reward, next_state in experiences:  # <1>\n",
    "        next_max = np.max(policy.state_action_table[next_state])  # <2>\n",
    "        value = policy.state_action_table[state][action]  # <3>\n",
    "        new_value = (1 - weight) * value + weight * \\\n",
    "                    (reward + discount_factor * next_max)  # <4>\n",
    "        policy.state_action_table[state][action] = new_value  # <5>\n",
    "# end::update_policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7fc03",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::train_policy[]\n",
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Training a policy by updating it with rollout experiences.\"\"\"\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)  # <1>\n",
    "        update_policy(policy, experiences, weight, discount_factor)  # <2>\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "trained_policy = train_policy(environment)  # <3>\n",
    "# end::train_policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9329a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::evaluate_policy[]\n",
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)  # <1>\n",
    "        steps += len(experiences)  # <2>\n",
    "\n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "    return steps / num_episodes\n",
    "\n",
    "\n",
    "evaluate_policy(environment, trained_policy)\n",
    "# end::evaluate_policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_policy_simulation[]\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "def create_policy():\n",
    "    env = Environment()\n",
    "    return Policy(env)  # <1>\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class SimulationActor(Simulation):  # <2>\n",
    "    \"\"\"Ray actor for a Simulation.\"\"\"\n",
    "    def __init__(self):\n",
    "        env = Environment()\n",
    "        super().__init__(env)\n",
    "# end::ray_policy_simulation[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc254782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_training[]\n",
    "@ray.remote\n",
    "def update_policy_task(policy, experiences_list):\n",
    "    \"\"\"Remote Ray task for updating a policy with experiences in parallel.\"\"\"\n",
    "    [update_policy(policy, ray.get(xp)) for xp in experiences_list]  # <1>\n",
    "    return policy\n",
    "\n",
    "\n",
    "def train_policy_parallel(num_episodes=1000, num_simulations=4):\n",
    "    \"\"\"Parallel policy training function.\"\"\"\n",
    "    policy = create_policy.remote()  # <2>\n",
    "    simulations = [SimulationActor.remote() for _ in range(num_simulations)]  # <3>\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = [sim.rollout.remote(policy) for sim in simulations]  # <4>\n",
    "        policy = update_policy_task.remote(policy, experiences)  # <5>\n",
    "\n",
    "    return ray.get(policy)  # <6>\n",
    "# end::ray_training[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ray_evaluation[]\n",
    "parallel_policy = train_policy_parallel()\n",
    "evaluate_policy(environment, parallel_policy)\n",
    "# end::ray_evaluation[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaaf094",
   "metadata": {},
   "source": [
    "# ![Task dependency](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_03/train_policy.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
