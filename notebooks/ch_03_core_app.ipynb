{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb6bfd0",
   "metadata": {},
   "source": [
    "# Chapter 3: Building Your First Distributed Application With Ray Core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1c204e",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_03_core_app.ipynb).\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_03_core_app.ipynb\">\n",
    "<img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "The book has been written for Ray 2.2.0, which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, for this chapter\n",
    "you can simply use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1540d697",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray>=2.1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2191e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb000a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class Discrete:\n",
    "    def __init__(self, num_actions: int):\n",
    "        \"\"\" Discrete action space for num_actions.\n",
    "        Discrete(4) can be used as encoding moving in\n",
    "        one of the cardinal directions.\n",
    "        \"\"\"\n",
    "        self.n = num_actions\n",
    "\n",
    "    def sample(self):\n",
    "        return random.randint(0, self.n - 1)\n",
    "\n",
    "\n",
    "space = Discrete(4)\n",
    "print(space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443af4f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class Environment:\n",
    "    def __init__(self,  *args, **kwargs):\n",
    "        self.seeker, self.goal = (0, 0), (4, 4)\n",
    "        self.info = {'seeker': self.seeker, 'goal': self.goal}\n",
    "\n",
    "        self.action_space = Discrete(4)\n",
    "        self.observation_space = Discrete(5*5)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset seeker position and return observations.\"\"\"\n",
    "        self.seeker = (0, 0)\n",
    "\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"Encode the seeker position as integer\"\"\"\n",
    "        return 5 * self.seeker[0] + self.seeker[1]\n",
    "\n",
    "    def get_reward(self):\n",
    "        \"\"\"Reward finding the goal\"\"\"\n",
    "        return 1 if self.seeker == self.goal else 0\n",
    "\n",
    "    def is_done(self):\n",
    "        \"\"\"We're done if we found the goal\"\"\"\n",
    "        return self.seeker == self.goal\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Take a step in a direction and return all available information.\"\"\"\n",
    "        if action == 0:  # move down\n",
    "            self.seeker = (min(self.seeker[0] + 1, 4), self.seeker[1])\n",
    "        elif action == 1:  # move left\n",
    "            self.seeker = (self.seeker[0], max(self.seeker[1] - 1, 0))\n",
    "        elif action == 2:  # move up\n",
    "            self.seeker = (max(self.seeker[0] - 1, 0), self.seeker[1])\n",
    "        elif action == 3:  # move right\n",
    "            self.seeker = (self.seeker[0], min(self.seeker[1] + 1, 4))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        obs = self.get_observation()\n",
    "        rew = self.get_reward()\n",
    "        done = self.is_done()\n",
    "        return obs, rew, done, self.info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        \"\"\"We override this method here so clear the output in Jupyter notebooks.\n",
    "        The previous implementation works well in the terminal, but does not clear\n",
    "        the screen in interactive environments.\n",
    "        \"\"\"\n",
    "        os.system('cls' if os.name == 'nt' else 'clear')\n",
    "        try:\n",
    "            from IPython.display import clear_output\n",
    "            clear_output(wait=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        grid = [['| ' for _ in range(5)] + [\"|\\n\"] for _ in range(5)]\n",
    "        grid[self.goal[0]][self.goal[1]] = '|G'\n",
    "        grid[self.seeker[0]][self.seeker[1]] = '|S'\n",
    "        print(''.join([''.join(grid_row) for grid_row in grid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8dc78b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "environment = Environment()\n",
    "\n",
    "while not environment.is_done():\n",
    "    random_action = environment.action_space.sample()\n",
    "    environment.step(random_action)\n",
    "    time.sleep(0.1)\n",
    "    environment.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8eecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A Policy suggests actions based on the current state.\n",
    "        We do this by tracking the value of each state-action pair.\n",
    "        \"\"\"\n",
    "        self.state_action_table = [\n",
    "            [0 for _ in range(env.action_space.n)]\n",
    "            for _ in range(env.observation_space.n)\n",
    "        ]\n",
    "        self.action_space = env.action_space\n",
    "\n",
    "    def get_action(self, state, explore=True, epsilon=0.1):\n",
    "        \"\"\"Explore randomly or exploit the best value currently available.\"\"\"\n",
    "        if explore and random.uniform(0, 1) < epsilon:\n",
    "            return self.action_space.sample()\n",
    "        return np.argmax(self.state_action_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de6020",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulation(object):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Simulates rollouts of an environment, given a policy to follow.\"\"\"\n",
    "        self.env = env\n",
    "\n",
    "    def rollout(self, policy, render=False, explore=True, epsilon=0.1):\n",
    "        \"\"\"Returns experiences for a policy rollout.\"\"\"\n",
    "        experiences = []\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = policy.get_action(state, explore, epsilon)\n",
    "            next_state, reward, done, info = self.env.step(action)\n",
    "            experiences.append([state, action, reward, next_state])\n",
    "            state = next_state\n",
    "            if render:\n",
    "                time.sleep(0.05)\n",
    "                self.env.render()\n",
    "\n",
    "        return experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3503b079",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "untrained_policy = Policy(environment)\n",
    "sim = Simulation(environment)\n",
    "\n",
    "exp = sim.rollout(untrained_policy, render=True, epsilon=1.0)\n",
    "for row in untrained_policy.state_action_table:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45eab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(policy, experiences, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Updates a given policy with a list of (state, action, reward, state)\n",
    "    experiences.\"\"\"\n",
    "    for state, action, reward, next_state in experiences:\n",
    "        next_max = np.max(policy.state_action_table[next_state])\n",
    "        value = policy.state_action_table[state][action]\n",
    "        new_value = (1 - weight) * value + weight * \\\n",
    "                    (reward + discount_factor * next_max)\n",
    "        policy.state_action_table[state][action] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d7fc03",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def train_policy(env, num_episodes=10000, weight=0.1, discount_factor=0.9):\n",
    "    \"\"\"Training a policy by updating it with rollout experiences.\"\"\"\n",
    "    policy = Policy(env)\n",
    "    sim = Simulation(env)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = sim.rollout(policy)\n",
    "        update_policy(policy, experiences, weight, discount_factor)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "trained_policy = train_policy(environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9329a87",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, num_episodes=10):\n",
    "    \"\"\"Evaluate a trained policy through rollouts.\"\"\"\n",
    "    simulation = Simulation(env)\n",
    "    steps = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = simulation.rollout(policy, render=True, explore=False)\n",
    "        steps += len(experiences)\n",
    "\n",
    "    print(f\"{steps / num_episodes} steps on average \"\n",
    "          f\"for a total of {num_episodes} episodes.\")\n",
    "\n",
    "    return steps / num_episodes\n",
    "\n",
    "\n",
    "evaluate_policy(environment, trained_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "class SimulationActor(Simulation):\n",
    "    \"\"\"Ray actor for a Simulation.\"\"\"\n",
    "    def __init__(self):\n",
    "        env = Environment()\n",
    "        super().__init__(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc254782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy_parallel(env, num_episodes=1000, num_simulations=4):\n",
    "    \"\"\"Parallel policy training function.\"\"\"\n",
    "    policy = Policy(env)\n",
    "    simulations = [SimulationActor.remote() for _ in range(num_simulations)]\n",
    "\n",
    "    policy_ref = ray.put(policy)\n",
    "    for _ in range(num_episodes):\n",
    "        experiences = [sim.rollout.remote(policy_ref) for sim in simulations]\n",
    "\n",
    "        while len(experiences) > 0:\n",
    "            finished, experiences = ray.wait(experiences)\n",
    "            for xp in ray.get(finished):\n",
    "                update_policy(policy, xp)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af8205",
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_policy = train_policy_parallel(environment)\n",
    "evaluate_policy(environment, parallel_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaaf094",
   "metadata": {},
   "source": [
    "# ![Task dependency](https://raw.githubusercontent.com/maxpumperla/learning_ray/main/notebooks/images/chapter_03/train_policy.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
