{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372c8436",
   "metadata": {},
   "source": [
    "# Chapter 6: Data Processing with Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7365d60",
   "metadata": {},
   "source": [
    "\n",
    "You can run this notebook directly in\n",
    "[Colab](https://colab.research.google.com/github/maxpumperla/learning_ray/blob/main/notebooks/ch_06_data_processing.ipynb).\n",
    "\n",
    "The book has been written for Ray 2.2.0,which at the time of writing has not\n",
    "officially been released yet. If you are reading this and this version is already\n",
    "available, you can install it using `pip install ray==2.2.0`. If not, you can\n",
    "use a nightly wheel (here for Python 3.7 on Linux):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a808bb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c6212",
   "metadata": {},
   "source": [
    "Should you not run this notebook in Colab and need another type of wheel, please\n",
    "refer to Ray's [installation instructions for nightlies](https://docs.ray.io/en/latest/ray-overview/installation.html#install-nightlies).\n",
    "\n",
    "For this chapter you will also need to install the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a6140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install \"ray[data]\"==2.0.0\n",
    "! pip install scikit-learn==1.1.2\n",
    "! pip install dask==2022.8.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40becbf9",
   "metadata": {},
   "source": [
    "\n",
    "To import utility files for this chapter, on Colab you will also have to clone\n",
    "the repo and copy the code files to the base path of the runtime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17aca7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maxpumperla/learning_ray\n",
    "%cp -r learning_ray/notebooks/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20447273",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_create[]\n",
    "import ray\n",
    "\n",
    "# Create a dataset containing integers in the range [0, 10000).\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Basic operations: show the size of the dataset, get a few samples, print the schema.\n",
    "print(ds.count())  # -> 10000\n",
    "print(ds.take(5))  # -> [0, 1, 2, 3, 4]\n",
    "print(ds.schema())  # -> <class 'int'>\n",
    "# end::ds_create[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665a2ff7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_read_write[]\n",
    "# Save the dataset to a local file and load it back.\n",
    "ray.data.range(10000).write_csv(\"local_dir\")\n",
    "ds = ray.data.read_csv(\"local_dir\")\n",
    "print(ds.count())\n",
    "# end::ds_read_write[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5847b020",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_transform[]\n",
    "ds1 = ray.data.range(10000)\n",
    "ds2 = ray.data.range(10000)\n",
    "ds3 = ds1.union(ds2)  # <1>\n",
    "print(ds3.count())  # -> 20000\n",
    "\n",
    "# Filter the combined dataset to only the even elements.\n",
    "ds3 = ds3.filter(lambda x: x % 2 == 0)  # <2>\n",
    "print(ds3.count())  # -> 10000\n",
    "print(ds3.take(5))  # -> [0, 2, 4, 6, 8]\n",
    "\n",
    "# Sort the filtered dataset.\n",
    "ds3 = ds3.sort()  # <3>\n",
    "print(ds3.take(5))  # -> [0, 0, 2, 2, 4]\n",
    "# end::ds_transform[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c5ecc7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_repartition[]\n",
    "ds1 = ray.data.range(10000)\n",
    "print(ds1.num_blocks())  # -> 200\n",
    "ds2 = ray.data.range(10000)\n",
    "print(ds2.num_blocks())  # -> 200\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.num_blocks())  # -> 400\n",
    "\n",
    "print(ds3.repartition(200).num_blocks())  # -> 200\n",
    "# end::ds_repartition[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f6cbde",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_1[]\n",
    "ds = ray.data.from_items([{\"id\": \"abc\", \"value\": 1}, {\"id\": \"def\", \"value\": 2}])\n",
    "print(ds.schema())  # -> id: string, value: int64\n",
    "# end::ds_schema_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4dcf23",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_2[]\n",
    "pandas_df = ds.to_pandas()  # pandas_df will inherit the schema from our Dataset.\n",
    "# end::ds_schema_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1d60e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_1[]\n",
    "ds = ray.data.range(10000).map(lambda x: x ** 2)\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bd5b81",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_2[]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5553d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ds_compute_3[]\n",
    "def load_model():\n",
    "    # Returns a dummy model for this example.\n",
    "    # In reality, this would likely load some model weights onto a GPU.\n",
    "    class DummyModel:\n",
    "        def __call__(self, batch):\n",
    "            return batch\n",
    "\n",
    "    return DummyModel()\n",
    "\n",
    "\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        # load_model() will only run once per actor that's started.\n",
    "        self._model = load_model()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._model(batch)\n",
    "\n",
    "\n",
    "ds.map_batches(MLModel, compute=\"actors\")\n",
    "# end::ds_compute_3[]\n",
    "\n",
    "\n",
    "cpu_intensive_preprocessing = lambda batch: batch\n",
    "gpu_intensive_inference = lambda batch: batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2350d3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_1[]\n",
    "ds = (ray.data.read_parquet(\"s3://my_bucket/input_data\")  # <1>\n",
    "      .map(cpu_intensive_preprocessing)  # <2>\n",
    "      .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)  # <3>\n",
    "      .repartition(10))  # <4>\n",
    "\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c1e05",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_2[]\n",
    "ds = (ray.data.read_parquet(\"s3://my_bucket/input_data\")\n",
    "      .window(blocks_per_window=5)\n",
    "      .map(cpu_intensive_preprocessing)\n",
    "      .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\n",
    "      .repartition(10))\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc33042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_1[]\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, alpha: float):\n",
    "        self._model = SGDClassifier(alpha=alpha)\n",
    "\n",
    "    def train(self, train_shard: ray.data.Dataset):\n",
    "        for i, epoch in enumerate(train_shard.iter_epochs()):\n",
    "            X, Y = zip(*list(epoch.iter_rows()))\n",
    "            self._model.partial_fit(X, Y, classes=[0, 1])\n",
    "\n",
    "        return self._model\n",
    "\n",
    "    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n",
    "        return self._model.score(X_test, Y_test)\n",
    "# end::parallel_sgd_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437935a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_2[]\n",
    "ALPHA_VALS = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012] \n",
    "\n",
    "print(f\"Starting {len(ALPHA_VALS)} training workers.\")\n",
    "workers = [TrainingWorker.remote(alpha) for alpha in ALPHA_VALS]\n",
    "# end::parallel_sgd_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46198d41",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_3[]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(  # <1>\n",
    "    *datasets.make_classification()\n",
    ")\n",
    "\n",
    "train_ds = ray.data.from_items(list(zip(X_train, Y_train)))  # <2>\n",
    "shards = (train_ds.repeat(10)  # <3>\n",
    "          .random_shuffle_each_window()  # <4>\n",
    "          .split(len(workers), locality_hints=workers))  # <5>\n",
    "\n",
    "ray.get([worker.train.remote(shard) for worker, shard in zip(workers, shards)])  # <6>\n",
    "# end::parallel_sgd_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9969a4ef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_5[]\n",
    "# Get validation results from each worker.\n",
    "print(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\n",
    "# end::parallel_sgd_5[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf0b4b7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_1[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "ray.init()  # Start or connect to Ray.\n",
    "enable_dask_on_ray()  # Enable the Ray scheduler backend for Dask.\n",
    "# end::dask_on_ray_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef41df5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_2[]\n",
    "import dask\n",
    "\n",
    "df = dask.datasets.timeseries()\n",
    "df = df[df.y > 0].groupby(\"name\").x.std()\n",
    "df.compute()  # Trigger the task graph to be evaluated.\n",
    "# end::dask_on_ray_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb6db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_3[]\n",
    "import ray\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Convert the Dataset to a Dask DataFrame.\n",
    "df = ds.to_dask()\n",
    "print(df.std().compute())  # -> 2886.89568\n",
    "\n",
    "# Convert the Dask DataFrame back to a Dataset.\n",
    "ds = ray.data.from_dask(df)\n",
    "print(ds.std())  # -> 2886.89568\n",
    "# end::dask_on_ray_3[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
