{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc564fa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_create[]\n",
    "import ray\n",
    "\n",
    "# Create a dataset containing integers in the range [0, 10000).\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Basic operations: show the size of the dataset, get a few samples, print the schema.\n",
    "print(ds.count())  # -> 10000\n",
    "print(ds.take(5))  # -> [0, 1, 2, 3, 4]\n",
    "print(ds.schema())  # -> <class 'int'>\n",
    "# end::ds_create[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a5cd0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_read_write[]\n",
    "# Save the dataset to a local file and load it back.\n",
    "ray.data.range(10000).write_csv(\"local_dir\")\n",
    "ds = ray.data.read_csv(\"local_dir\")\n",
    "print(ds.count())\n",
    "# end::ds_read_write[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565225f5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_transform[]\n",
    "# Basic transformations: join two datasets, filter, and sort.\n",
    "ds1 = ray.data.range(10000)\n",
    "ds2 = ray.data.range(10000)\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.count())  # -> 20000\n",
    "\n",
    "# Filter the combined dataset to only the even elements.\n",
    "ds3 = ds3.filter(lambda x: x % 2 == 0)\n",
    "print(ds3.count())  # -> 10000\n",
    "print(ds3.take(5))  # -> [0, 2, 4, 6, 8]\n",
    "\n",
    "# Sort the filtered dataset.\n",
    "ds3 = ds3.sort()\n",
    "print(ds3.take(5))  # -> [0, 0, 2, 2, 4]\n",
    "# end::ds_transform[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91804f1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_repartition[]\n",
    "ds1 = ray.data.range(10000)\n",
    "print(ds1.num_blocks())  # -> 200\n",
    "ds2 = ray.data.range(10000)\n",
    "print(ds2.num_blocks())  # -> 200\n",
    "ds3 = ds1.union(ds2)\n",
    "print(ds3.num_blocks())  # -> 400\n",
    "\n",
    "print(ds3.repartition(200).num_blocks())  # -> 200\n",
    "# end::ds_repartition[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e304828",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_1[]\n",
    "ds = ray.data.from_items([{\"id\": \"abc\", \"value\": 1}, {\"id\": \"def\", \"value\": 2}])\n",
    "print(ds.schema())  # -> id: string, value: int64\n",
    "# end::ds_schema_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a490c0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_schema_2[]\n",
    "pandas_df = ds.to_pandas()  # pandas_df will inherit the schema from our Dataset.\n",
    "# end::ds_schema_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bda5406",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_1[]\n",
    "ds = ray.data.range(10000).map(lambda x: x ** 2)\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367f18d0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_compute_2[]\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ds = ray.data.range(10000).map_batches(lambda batch: np.square(batch).tolist())\n",
    "ds.take(5)  # -> [0, 1, 4, 9, 16]\n",
    "# end::ds_compute_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb53c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::ds_compute_3[]\n",
    "def load_model():\n",
    "    # Returns a dummy model for this example.\n",
    "    # In reality, this would likely load some model weights onto a GPU.\n",
    "    class DummyModel:\n",
    "        def __call__(self, batch):\n",
    "            return batch\n",
    "\n",
    "    return DummyModel()\n",
    "\n",
    "\n",
    "class MLModel:\n",
    "    def __init__(self):\n",
    "        # load_model() will only run once per actor that's started.\n",
    "        self._model = load_model()\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        return self._model(batch)\n",
    "\n",
    "\n",
    "ds.map_batches(MLModel, compute=\"actors\")\n",
    "# end::ds_compute_3[]\n",
    "\n",
    "\n",
    "cpu_intensive_preprocessing = lambda batch: batch\n",
    "gpu_intensive_inference = lambda batch: batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082aef2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_1[]\n",
    "ds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n",
    "        .map(cpu_intensive_preprocessing)\\\n",
    "        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n",
    "        .repartition(10)\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484d5915",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::ds_pipeline_2[]\n",
    "ds = ray.data.read_parquet(\"s3://my_bucket/input_data\")\\\n",
    "        .window(blocks_per_window=5)\\\n",
    "        .map(cpu_intensive_preprocessing)\\\n",
    "        .map_batches(gpu_intensive_inference, compute=\"actors\", num_gpus=1)\\\n",
    "        .repartition(10)\n",
    "ds.write_parquet(\"s3://my_bucket/output_predictions\")\n",
    "# end::ds_pipeline_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38798e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_1[]\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class TrainingWorker:\n",
    "    def __init__(self, alpha: float):\n",
    "        self._model = SGDClassifier(alpha=alpha)\n",
    "\n",
    "    def train(self, train_shard: ray.data.Dataset):\n",
    "        for i, epoch in enumerate(train_shard.iter_epochs()):\n",
    "            X, Y = zip(*list(epoch.iter_rows()))\n",
    "            self._model.partial_fit(X, Y, classes=[0, 1])\n",
    "\n",
    "        return self._model\n",
    "\n",
    "    def test(self, X_test: np.ndarray, Y_test: np.ndarray):\n",
    "        return self._model.score(X_test, Y_test)\n",
    "# end::parallel_sgd_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0121c3d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_2[]\n",
    "ALPHA_VALS = [0.00008, 0.00009, 0.0001, 0.00011, 0.00012] \n",
    "\n",
    "print(f\"Starting {len(ALPHA_VALS)} training workers.\")\n",
    "workers = [TrainingWorker.remote(alpha) for alpha in ALPHA_VALS]\n",
    "# end::parallel_sgd_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30126c6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_3[]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(  # <1>\n",
    "    *datasets.make_classification()\n",
    ")\n",
    "\n",
    "train_ds = ray.data.from_items(list(zip(X_train, Y_train)))  # <2>\n",
    "shards = train_ds.repeat(10)\\\n",
    "                 .random_shuffle_each_window()\\\n",
    "                 .split(len(workers), locality_hints=workers)\n",
    "\n",
    "ray.get([worker.train.remote(shard) for worker, shard in zip(workers, shards)])  # <3>\n",
    "# end::parallel_sgd_3[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a9146",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::parallel_sgd_5[]\n",
    "# Get validation results from each worker.\n",
    "print(ray.get([worker.test.remote(X_test, Y_test) for worker in workers]))\n",
    "# end::parallel_sgd_5[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213adac6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_1[]\n",
    "import ray\n",
    "from ray.util.dask import enable_dask_on_ray\n",
    "\n",
    "ray.init()  # Start or connect to Ray.\n",
    "enable_dask_on_ray()  # Enable the Ray scheduler backend for Dask.\n",
    "# end::dask_on_ray_1[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dc2b8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_2[]\n",
    "import dask\n",
    "\n",
    "df = dask.datasets.timeseries()\n",
    "df = df[df.y > 0].groupby(\"name\").x.std()\n",
    "df.compute()  # Trigger the task graph to be evaluated.\n",
    "# end::dask_on_ray_2[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead357eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag::dask_on_ray_3[]\n",
    "import ray\n",
    "ds = ray.data.range(10000)\n",
    "\n",
    "# Convert the Dataset to a Dask DataFrame.\n",
    "df = ds.to_dask()\n",
    "print(df.std().compute())  # -> 2886.89568\n",
    "\n",
    "# Convert the Dask DataFrame back to a Dataset.\n",
    "ds = ray.data.from_dask(df)\n",
    "print(ds.std())  # -> 2886.89568\n",
    "# end::dask_on_ray_3[]"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
